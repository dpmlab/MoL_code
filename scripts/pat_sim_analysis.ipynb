{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import deepdish as dd\n",
    "import string\n",
    "try:\n",
    "    os.chdir('/data/MoL_clean/scripts')\n",
    "except:\n",
    "    pass\n",
    "import util\n",
    "# util has some variables in them\n",
    "# import GLM_helper as gh\n",
    "import random \n",
    "\n",
    "import scipy.stats as stats\n",
    "import glob\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "from scipy import stats, linalg\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sub2subj = {\"sub-01\":\"subj001\", \"sub-02\":\"subj002\",\"sub-03\":\"subj003\",\"sub-04\":\"subj005\",\n",
    "              \"sub-05\":\"subj006\", \"sub-06\":\"subj007\", \"sub-07\":\"subj008\", \"sub-08\":\"subj009\", \n",
    "           \"sub-09\":\"subj010\",\"sub-10\":\"subj011\",\"sub-11\":\"subj013\", \"sub-12\":\"subj014\", \n",
    "            \"sub-13\":\"subj017\", \"sub-14\":\"subj018\", \"sub-15\":\"subj019\", \"sub-16\":\"subj020\",\n",
    "           \"sub-17\":\"subj021\", \"sub-18\":\"subj022\", \"sub-19\":\"subj023\", \"sub-20\":\"subj024\",'sub-21':'subj025',\n",
    "           'sub-22':'subj026','sub-23':'subj027','sub-24':'subj029','sub-25':'subj031'}\n",
    "ses2w = {\"ses-01\":\"W2\", \"ses-02\":\"W4D1\", \"ses-03\":\"W4D2\"}\n",
    "\n",
    "os.chdir('/data/MoL_clean/scripts')\n",
    "nv = 40962\n",
    "\n",
    "subjects = ['sub-%.2d'%s for s in range(1,26)]\n",
    "sessions = ['ses-%.2d'%s for s in range(1,3)]\n",
    "runs = ['run-%.2d'%s for s in range(1,3)]\n",
    "TR = 1.5\n",
    "nTRs = {'Item':302, 'Loci':302, 'Encode':355}\n",
    "\n",
    "nTRs_w4d2 = {'Item': 156, 'Loci': 156, 'Encode': 182}\n",
    "SL_lh = list(dd.io.load('SLlist_verydense.lh.h5').values())\n",
    "SL_rh = list(dd.io.load('SLlist_verydense.rh.h5').values())\n",
    "ag = list(dd.io.load('ROIs/Ang_verts.h5').values())\n",
    "pmc = list(dd.io.load('ROIs/PMC_verts.h5').values())\n",
    "mPFC = list(dd.io.load('ROIs/mPFC_verts.h5').values())\n",
    "ROIs = {'ag':ag, 'pmc':pmc, 'mpfc':mPFC}\n",
    "SLlist = {'L':SL_lh, \"R\": SL_rh}\n",
    "nSL_L = len(SLlist['L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_xs(string_list):\n",
    "    return [s for s in string_list if s!='x']\n",
    "\n",
    "\n",
    "def add_numbers_to_duplicates(string_list):\n",
    "    count = {}\n",
    "    new_list = []\n",
    "    \n",
    "    for item in string_list:\n",
    "        if item in count and item!='x':\n",
    "            count[item] += 1\n",
    "            new_item = f\"{item} {count[item]}\"\n",
    "        else:\n",
    "            count[item] = 1\n",
    "            new_item = item\n",
    "        \n",
    "        new_list.append(new_item.lower())\n",
    "        \n",
    "    return new_list\n",
    "\n",
    "\n",
    "def partial_corr(C, desired_i= None, desired_j=None):\n",
    "    \"\"\"\n",
    "    Returns the sample linear partial correlation coefficients between pairs of variables in C, controlling \n",
    "    for the remaining variables in C.\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n, p)\n",
    "        Array with the different variables. Each column of C is taken as a variable\n",
    "    Desired_i, desired_j: int\n",
    "        If only wants to calculate the partial correlation between desired_i and desired_j, set them to be the index of the variables\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    P : array-like, shape (p, p)\n",
    "        P[i, j] contains the partial correlation of C[:, i] and C[:, j] controlling\n",
    "        for the remaining variables in C\n",
    "    \"\"\"\n",
    "    \n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p), dtype=np.float)\n",
    "    if desired_i is not None and desired_j is not None:\n",
    "        P_corr[desired_i, desired_j] = 1\n",
    "        P_corr[desired_j, desired_i] = 1\n",
    "        idx = np.ones(p, dtype=np.bool)\n",
    "        idx[desired_i] = False\n",
    "        idx[desired_j] = False\n",
    "        beta_i = linalg.lstsq(C[:, idx], C[:, desired_j])[0]\n",
    "        beta_j = linalg.lstsq(C[:, idx], C[:, desired_i])[0]\n",
    "\n",
    "        res_j = C[:, desired_j] - C[:, idx].dot( beta_i)\n",
    "        res_i = C[:, desired_i] - C[:, idx].dot(beta_j)\n",
    "\n",
    "        corr = stats.pearsonr(res_i, res_j)[0]\n",
    "        P_corr[desired_i, desired_j] = corr\n",
    "        P_corr[desired_j, desired_i] = corr\n",
    "        P_corr = corr\n",
    "    else:\n",
    "        for i in range(p):\n",
    "            P_corr[i, i] = 1\n",
    "            for j in range(i+1, p):\n",
    "                idx = np.ones(p, dtype=np.bool)\n",
    "                idx[i] = False\n",
    "                idx[j] = False\n",
    "                beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n",
    "                beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n",
    "\n",
    "                res_j = C[:, j] - C[:, idx].dot( beta_i)\n",
    "                res_i = C[:, i] - C[:, idx].dot(beta_j)\n",
    "                \n",
    "                corr = stats.pearsonr(res_i, res_j)[0]\n",
    "                P_corr[i, j] = corr\n",
    "                P_corr[j, i] = corr\n",
    "            \n",
    "    return P_corr\n",
    "\n",
    "def SLtoVox(D, SLlist, nv, zeronan=True):\n",
    "    # D is dict of L, R, with N x arbitrary dims\n",
    "    # SLlist is dict of L, R list of length N, with vertices for each SL\n",
    "\n",
    "    Dvox = dict()\n",
    "    Dcount = dict()\n",
    "    for hem in ['L', 'R']:\n",
    "        Dvox[hem] = np.zeros((nv,)+ D[hem].shape[1:])\n",
    "        Dcount[hem] = np.zeros((nv,)+(1,)*len(D[hem].shape[1:]))\n",
    "        for i in range(len(SLlist[hem])):\n",
    "            Dvox[hem][SLlist[hem][i]] += D[hem][i]\n",
    "            Dcount[hem][SLlist[hem][i]] += 1\n",
    "\n",
    "        Dcount[hem][Dcount[hem] == 0] = np.nan\n",
    "        Dvox[hem] = Dvox[hem] / Dcount[hem]\n",
    "\n",
    "        if zeronan:\n",
    "            Dvox[hem][np.isnan(Dvox[hem])] = 0\n",
    "\n",
    "    return Dvox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this version get beta maps for locus and item without averaging the two loci and item runs.\n",
    "def get_beta_dicts_li(sub,ses,hippo=False):\n",
    "    task = 'Item'\n",
    "    # load item \n",
    "    item_filenames = sorted(glob.glob(f'../behavioral/{sub2subj[sub]}/{ses2w[ses]}/*{task.lower()}*.csv'))\n",
    "    item_words_lists = [[w for w in pd.read_csv(item_filenames[0])['Word'] if w is not np.nan],[w for w in pd.read_csv(item_filenames[1])['Word'] if w is not np.nan]]\n",
    "    item_beta_dict = {'lh':{w:[] for w in item_words_lists[0]}, 'rh':{w:[] for w in item_words_lists[0]},'anterior_hipp':{w:[] for w in item_words_lists[0]},'posterior_hipp':{w:[] for w in item_words_lists[0]}}\n",
    "    task = 'Loci'\n",
    "    # load loci lists, with loci names in two lists. Create a dictionary accordingly taking the union of the two lists\n",
    "    loci_lists = [add_numbers_to_duplicates(list(pd.read_excel('../sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name='%sloci1'%ses2w[ses].lower())['spoken_loci'])),\n",
    "                        add_numbers_to_duplicates(list(pd.read_excel('../sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name='%sloci2'%ses2w[ses].lower())['spoken_loci'])),]\n",
    "    loci_beta_dict = {\"lh\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))}, \"rh\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},\"anterior_hipp\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},\"posterior_hipp\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))}}\n",
    "    \n",
    "    # create dictionary of locus and item for each of the two runs\n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp']:\n",
    "        run = 'run-01'\n",
    "        loci_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, l in enumerate(loci_lists[0]):\n",
    "            loci_beta_dict[hem][l].append(loci_fmri[:,i])\n",
    "        run = 'run-02'\n",
    "        loci_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, l in enumerate(loci_lists[1]):\n",
    "            loci_beta_dict[hem][l].append(loci_fmri[:,i])\n",
    "    # average locus rep if spoken in both runs\n",
    "\n",
    "    \n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp']: \n",
    "        run = 'run-01'\n",
    "        task = 'Item'\n",
    "        item_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, w in enumerate(item_words_lists[0]):\n",
    "            item_beta_dict[hem][w].append(item_fmri[:,i])\n",
    "        run = 'run-02'\n",
    "        item_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, w in enumerate(item_words_lists[1]):\n",
    "            item_beta_dict[hem][w].append(item_fmri[:,i])\n",
    "\n",
    "    return loci_beta_dict, item_beta_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta_dicts(sub,ses,hippo=False):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with beta maps for loci, item, encoding, and retrieval for a given subject and session.\n",
    "    \"\"\"\n",
    "    task = 'Item'\n",
    "    # load item \n",
    "    item_filenames = sorted(glob.glob(f'../behavioral/{sub2subj[sub]}/{ses2w[ses]}/*{task.lower()}*.csv'))\n",
    "    item_words_lists = [[w for w in pd.read_csv(item_filenames[0])['Word'] if w is not np.nan],[w for w in pd.read_csv(item_filenames[1])['Word'] if w is not np.nan]]\n",
    "    item_beta_dict = {'lh':{w:[] for w in item_words_lists[0]}, 'rh':{w:[] for w in item_words_lists[0]},'anterior_hipp':{w:[] for w in item_words_lists[0]},'posterior_hipp':{w:[] for w in item_words_lists[0]}}\n",
    "    task = 'Loci'\n",
    "    # load loci lists, with loci names in two lists. Create a dictionary accordingly taking the union of the two lists\n",
    "    loci_lists = [add_numbers_to_duplicates(list(pd.read_excel('../sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name='%sloci1'%ses2w[ses].lower())['spoken_loci'])),\n",
    "                        add_numbers_to_duplicates(list(pd.read_excel('../sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name='%sloci2'%ses2w[ses].lower())['spoken_loci'])),]\n",
    "    loci_beta_dict = {\"lh\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))}, \"rh\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},\"anterior_hipp\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},\"posterior_hipp\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))}}\n",
    "    \n",
    "    # create dictionary of locus and item for each of the two runs\n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp']:\n",
    "        run = 'run-01'\n",
    "        loci_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, l in enumerate(loci_lists[0]):\n",
    "            loci_beta_dict[hem][l].append(loci_fmri[:,i])\n",
    "        run = 'run-02'\n",
    "        loci_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, l in enumerate(loci_lists[1]):\n",
    "            loci_beta_dict[hem][l].append(loci_fmri[:,i])\n",
    "    # average locus rep if spoken in both runs\n",
    "        for l in loci_beta_dict[hem]:\n",
    "            if len(loci_beta_dict[hem][l]) == 2:\n",
    "                loci_beta_dict[hem][l] = np.mean(loci_beta_dict[hem][l],axis=0)\n",
    "            else:\n",
    "                loci_beta_dict[hem][l] = loci_beta_dict[hem][l][0]\n",
    "    \n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp']: \n",
    "        run = 'run-01'\n",
    "        task = 'Item'\n",
    "        item_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, w in enumerate(item_words_lists[0]):\n",
    "            item_beta_dict[hem][w].append(item_fmri[:,i])\n",
    "        run = 'run-02'\n",
    "        item_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, w in enumerate(item_words_lists[1]):\n",
    "            item_beta_dict[hem][w].append(item_fmri[:,i])\n",
    "\n",
    "        for w in item_beta_dict[hem]:\n",
    "            item_beta_dict[hem][w] = np.mean(item_beta_dict[hem][w],axis=0)\n",
    "\n",
    "    retrieve_beta_dict = {'lh':{},'rh':{},'anterior_hipp':{},'posterior_hipp':{}}\n",
    "    encode_beta_dict = {'lh':{},'rh':{},'anterior_hipp':{},'posterior_hipp':{}}\n",
    "    run = 'run-01'\n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp']:\n",
    "        recall_sheet = pd.read_excel('../sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name=ses2w[ses].lower())\n",
    "        recall_sheet['retrieval'] = recall_sheet['retrieval'].apply(lambda x: x.split(' ')[0])\n",
    "        recall_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%('retrieve',sub,ses,run,hem))\n",
    "        encode_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%('encode',sub,ses,run,hem))\n",
    "        if ses != 'ses-03':\n",
    "            encode_pairs = [recall_sheet['loci'][r]+'-'+recall_sheet['encode'][r] for r in range(len(recall_sheet))][:40]\n",
    "        else:\n",
    "            encode_pairs = [recall_sheet['loci'][r]+'-'+recall_sheet['encode'][r] for r in range(len(recall_sheet))][:20]\n",
    "        valid_encode_pairs = [p for p in encode_pairs if p[0]!='x']\n",
    "        for r, pair in enumerate(valid_encode_pairs):\n",
    "            encode_beta_dict[hem][pair.lower()] = encode_fmri[:,r]\n",
    "        ret_idx = 0\n",
    "        for r in range(len(recall_sheet)):\n",
    "            if recall_sheet['spoken_loci'][r]!='x':\n",
    "                retrieve_beta_dict[hem][(recall_sheet['spoken_loci'][r]+'-'+recall_sheet['retrieval'][r]).lower()] = recall_fmri[:,ret_idx]\n",
    "                ret_idx+=1\n",
    "    return loci_beta_dict, item_beta_dict, encode_beta_dict, retrieve_beta_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_residuals(C, dv):\n",
    "    \"\"\"\n",
    "    Returns the residuals of the dv(th) column of matrix C, accounting for all variances explained by other columns\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n, p)\n",
    "        Array with the different variables. Each column of C is taken as a variable\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : array-like, shape (n,)\n",
    "        Residual of dv\n",
    "    \"\"\"\n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    idx = np.ones(p, dtype=np.bool)\n",
    "    idx[dv] = False\n",
    "    beta = linalg.lstsq(C[:, idx], C[:, dv])[0]\n",
    "\n",
    "    res = C[:, dv] - C[:, idx].dot(beta)\n",
    "            \n",
    "    return res\n",
    "\n",
    "\n",
    "def get_residual(target, var1, var2):\n",
    "    # Create a 2D array for the independent variables\n",
    "    X = np.column_stack((var1, var2))\n",
    "    \n",
    "    # Fit a linear regression model with the independent variables\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, target)\n",
    "    \n",
    "    # Get the predicted values for the target variable based on the model\n",
    "    predicted = model.predict(X)\n",
    "    \n",
    "    # Calculate the residuals by subtracting the predicted values from the original target values\n",
    "    residuals = target - predicted\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "def calculate_weights(x, y, z, A):\n",
    "    \"\"\"\n",
    "    Calculate the weights of variables x, y, z in predicting each timepoint of matrix A.\n",
    "\n",
    "    Parameters:\n",
    "    x, y, z: Arrays of shape (nv,) representing the variables in each vertices in the ROI.\n",
    "    A: Matrix of shape (nv, nTR) where each column is a timepoint.\n",
    "\n",
    "    Returns:\n",
    "    weights: A matrix of shape (nTR, 3) containing the weights for each variable for each timepoint.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine x, y, z into a single matrix\n",
    "    predictors = np.column_stack((x, y, z))\n",
    "    try:\n",
    "        num_TRs = A.shape[1]\n",
    "            # Initialize an array to store weights\n",
    "        weights = np.zeros((num_TRs, 3))\n",
    "\n",
    "        # Loop over each timepoint\n",
    "        for i in range(num_TRs):\n",
    "            # Create a linear regression model\n",
    "            model = LinearRegression()\n",
    "\n",
    "            # Fit the model\n",
    "            model.fit(predictors, A[:, i])\n",
    "\n",
    "            # Store the weights\n",
    "            weights[i, :] = model.coef_\n",
    "    except:\n",
    "        num_TRs = 1\n",
    "        model = LinearRegression()\n",
    "        model.fit(predictors, A)\n",
    "        weights = model.coef_\n",
    "    \n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diag_diff_perm(corrmat, nPerm=1000):\n",
    "    '''\n",
    "    return the (diagonal mean - off-diagonal) mean in a correlation matrix, including the all the means \n",
    "    if the correlation matrix is randomly shuffled\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corrmat : array-like, shape (n_roi, n, n)\n",
    "        Array with the different variables. \n",
    "        n_roi: number of rois\n",
    "        n: the dimension of the correlation matrix\n",
    "    nPerm: int, number of permutations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    diag_diff : array-like, shape (nPerm+1,)\n",
    "        Residual of dv\n",
    "    '''\n",
    "    diag_diff = np.zeros((corrmat.shape[0], nPerm+1))\n",
    "    for i in range(corrmat.shape[0]):       \n",
    "        corr_i = corrmat[i, :, :].copy()     \n",
    "        corr_perm = corr_i[np.ix_(np.arange(1,corr_i.shape[1]), np.arange(1,corr_i.shape[1]))].copy()\n",
    "        np.random.seed(0)\n",
    "        for p in range(nPerm+1):\n",
    "            diag_diff[i, p] = corr_perm[np.eye(corr_perm.shape[0],dtype=bool)].mean() - corr_perm[~np.eye(corr_perm.shape[0],dtype=bool)].mean()\n",
    "            corr_perm = corr_perm[np.random.permutation(corr_perm.shape[0]), :].copy()\n",
    "    return diag_diff\n",
    "\n",
    "\n",
    "\n",
    "def generate_shuffled_indexes(indexes_to_shuffle, num_permutation):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a list of shuffled indexes based on the provided list and number of permutations.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_index_container = []\n",
    "    for p in range(num_permutation):\n",
    "        shuffled_list = indexes_to_shuffle[:]\n",
    "        random.shuffle(shuffled_list)\n",
    "        random_index_container.append(shuffled_list)\n",
    "    return random_index_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding recall partial correlation (version 1, get SL map of partial correlation between encoding and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates correlation matrix between tasks (L-L, I-I, I-E, L-E, I-R, L-R\n",
    "# with same item/loci/combination) for each subject in each session\n",
    "# also get weights for L, I, E (residual, removing I and L), predicting R\n",
    "# For the weights, generate a null distributino with permutation, and get the z-score, and the \n",
    "# results of a null distribution (weight_store_by_sl_shuffled)\n",
    "\n",
    "def get_sub_corr_SL(sub):\n",
    "    sessions = ['ses-01','ses-02','ses-03']\n",
    "    for ses in sessions:\n",
    "        weight_store_by_sl = {}\n",
    "        weight_store_by_sl_shuffled = {}\n",
    "        weight_store_by_sl_z = {}\n",
    "        e_r_semipartial = {}\n",
    "        l_e_corr = {}\n",
    "        l_r_corr = {}\n",
    "        i_e_corr = {}\n",
    "        i_r_corr = {}\n",
    "        l_l_corr = {}\n",
    "        i_i_corr = {}\n",
    "        \n",
    "        # get beta dictionaries for loci and item, without averaging the two runs\n",
    "        loci_beta_dict_noaverage, item_beta_dict_noaverage = get_beta_dicts_li(sub,ses)\n",
    "\n",
    "        # get beta dictionaries for loci, item, encode, and retrieve, averaging two loci and item runs\n",
    "        loci_beta_dict, item_beta_dict, encode_beta_dict, retrieve_beta_dict = get_beta_dicts(sub,ses)\n",
    "\n",
    "        for hem in ['lh','rh']:\n",
    "            # load the SL list for the current hemisphere\n",
    "            sl = dd.io.load('SLlist_verydense.'+hem+'.h5')\n",
    "            # initialize the dictionaries to store the weights and correlation matrices\n",
    "            weight_store_by_sl[hem] = np.zeros((len(sl),3))\n",
    "            weight_store_by_sl_shuffled[hem] = np.zeros((len(sl),3))\n",
    "            weight_store_by_sl_z[hem] = np.zeros((len(sl),3))\n",
    "            e_r_semipartial[hem] = []\n",
    "            l_e_corr[hem] = []\n",
    "            l_r_corr[hem] = []\n",
    "            i_e_corr[hem] = []\n",
    "            i_r_corr[hem] = []\n",
    "            l_l_corr[hem] = []\n",
    "            i_i_corr[hem] = []\n",
    "\n",
    "            \n",
    "            for sl_i, l in enumerate(sl):\n",
    "                # initialize lists to store the weights and correlation matrices for the current SL\n",
    "                loci_weights = []\n",
    "                item_weights = []\n",
    "                encode_weights = []\n",
    "                retrieve_weights = []\n",
    "                retrieve_all= []\n",
    "                encode_all = []\n",
    "                encode_residual_all = []\n",
    "                locus_all = []\n",
    "                item_all = []\n",
    "\n",
    "                loci_weights_z = []\n",
    "                item_weights_z = []\n",
    "                encode_weights_z = []\n",
    "                \n",
    "                loci_weights_shuffled = []\n",
    "                item_weights_shuffled = []\n",
    "                encode_weights_shuffled = []\n",
    "\n",
    "                locus_weights1 = []\n",
    "                locus_weights2 = []\n",
    "                item_weights1 = []\n",
    "                item_weights2 = []\n",
    "\n",
    "                valid_pairs = []\n",
    "\n",
    "                # get valid pairs of locus and item for the current SL\n",
    "                for i, key in enumerate(retrieve_beta_dict[hem]):\n",
    "                    locus,word = key.split('-')\n",
    "                    if locus in loci_beta_dict[hem] and word != 'x' and locus+'-'+word in encode_beta_dict[hem]:\n",
    "                        valid_pairs.append(key)\n",
    "                # generate 100 permutations of the valid pairs with shuffled \n",
    "                shuffled_keys = generate_shuffled_indexes(valid_pairs, 100)\n",
    "                for i, key in enumerate(valid_pairs):\n",
    "                    locus,word = key.split('-')\n",
    "                    if locus in loci_beta_dict[hem] and word != 'x' and locus+'-'+word in encode_beta_dict[hem]:\n",
    "                        locus_rep = loci_beta_dict[hem][locus][sl[l]]\n",
    "                        item_rep = item_beta_dict[hem][word][sl[l]]\n",
    "                        encode_rep = encode_beta_dict[hem][locus+'-'+word][sl[l]]\n",
    "                        retrieve_rep = retrieve_beta_dict[hem][key][sl[l]]\n",
    "\n",
    "                        # look at locus representation limited to correctly retrieved locus/item\n",
    "                        locus_rep_noaverage = loci_beta_dict_noaverage[hem][locus]\n",
    "                        item_rep_noaverage = item_beta_dict_noaverage[hem][word]\n",
    "\n",
    "                        encode_rep_residual = get_residual(encode_rep, locus_rep, item_rep)\n",
    "                        encode_residual_all.append(encode_rep_residual)\n",
    "\n",
    "                        retrieve_all.append(retrieve_rep)\n",
    "                        encode_all.append(encode_rep)\n",
    "                        locus_all.append(locus_rep)\n",
    "                        item_all.append(item_rep)\n",
    "\n",
    "                        # for locus-locus and item-item correlations, store the weights for the two runs\n",
    "                        item_weights1.append(item_rep_noaverage[0][sl[l]])\n",
    "                        item_weights2.append(item_rep_noaverage[1][sl[l]])\n",
    "                        if len(locus_rep_noaverage) == 2:\n",
    "                            locus_weights1.append(locus_rep_noaverage[0][sl[l]])\n",
    "                            locus_weights2.append(locus_rep_noaverage[1][sl[l]])\n",
    "                        \n",
    "                        # run regression to get the weights of locus, item, encoding residual, for the SL\n",
    "                        l_weights,i_weights,e_weights = calculate_weights(locus_rep, item_rep, encode_rep_residual, retrieve_rep)\n",
    "                        loci_weights.append(l_weights)\n",
    "                        item_weights.append(i_weights)  \n",
    "                        encode_weights.append(e_weights)\n",
    "\n",
    "                        loci_weights_shuffled_item = []\n",
    "                        item_weights_shuffled_item = []\n",
    "                        encode_weights_shuffled_item = []\n",
    "                        # for each of the 100 permutations, get the weights for the shuffled keys\n",
    "                        for p in range(100):\n",
    "                            retrieve_rep_shuffled = retrieve_beta_dict[hem][shuffled_keys[p][i]][sl[l]]\n",
    "                            l_weights_shuffled,i_weights_shuffled,e_weights_shuffled = calculate_weights(locus_rep, item_rep, encode_rep_residual, retrieve_rep_shuffled)\n",
    "                            loci_weights_shuffled_item.append(l_weights_shuffled)\n",
    "                            item_weights_shuffled_item.append(i_weights_shuffled)\n",
    "                            encode_weights_shuffled_item.append(e_weights_shuffled)\n",
    "                        # compute the z-scores for the weights, comparing the weight when predicting the right vs. \n",
    "                        # wrong retrieval representation from the same locus-item-encoding_residual\n",
    "                        loci_weights_z.append((l_weights-np.mean(loci_weights_shuffled_item))/np.std(loci_weights_shuffled_item)) \n",
    "                        item_weights_z.append((i_weights-np.mean(item_weights_shuffled_item))/np.std(item_weights_shuffled_item))\n",
    "                        encode_weights_z.append((e_weights-np.mean(encode_weights_shuffled_item))/np.std(encode_weights_shuffled_item))\n",
    "\n",
    "                        loci_weights_shuffled.append(np.mean(loci_weights_shuffled_item))\n",
    "                        item_weights_shuffled.append(np.mean(item_weights_shuffled_item))\n",
    "                        encode_weights_shuffled.append(np.mean(encode_weights_shuffled_item))\n",
    "\n",
    "                # store the weights, the z-scores, and the shuffled weights for the current SL. \n",
    "                weight_store_by_sl[hem][sl_i,0] = np.mean(np.nan_to_num(loci_weights))\n",
    "                weight_store_by_sl[hem][sl_i,1] = np.mean(np.nan_to_num(item_weights))\n",
    "                weight_store_by_sl[hem][sl_i,2] = np.mean(np.nan_to_num(encode_weights))\n",
    "                \n",
    "                weight_store_by_sl_z[hem][sl_i,0] = np.mean(np.nan_to_num(loci_weights_z))\n",
    "                weight_store_by_sl_z[hem][sl_i,1] = np.mean(np.nan_to_num(item_weights_z))\n",
    "                weight_store_by_sl_z[hem][sl_i,2] = np.mean(np.nan_to_num(encode_weights_z))\n",
    "\n",
    "                weight_store_by_sl_shuffled[hem][sl_i,0] = np.mean(np.nan_to_num(loci_weights_shuffled))\n",
    "                weight_store_by_sl_shuffled[hem][sl_i,1] = np.mean(np.nan_to_num(item_weights_shuffled))\n",
    "                weight_store_by_sl_shuffled[hem][sl_i,2] = np.mean(np.nan_to_num(encode_weights_shuffled))\n",
    "\n",
    "                # compute the correlation matrices for the current SL\n",
    "\n",
    "\n",
    "                le_corrmat = np.corrcoef(locus_all, encode_all)[len(locus_all):, :len(encode_all)]\n",
    "                lr_corrmat = np.corrcoef(locus_all, retrieve_all)[len(locus_all):, :len(retrieve_all)]\n",
    "                ie_corrmat = np.corrcoef(item_all, encode_all)[len(item_all):, :len(encode_all)]\n",
    "                ir_corrmat = np.corrcoef(item_all, retrieve_all)[len(item_all):, :len(retrieve_all)]\n",
    "\n",
    "                # compute the correlation matrices for locus-locus and item-item\n",
    "                ll_corrmat = np.corrcoef(locus_weights1,locus_weights2)[len(locus_weights1):, :len(locus_weights2)]\n",
    "                ii_corrmat = np.corrcoef(item_weights1,item_weights2)[len(item_weights1):, :len(item_weights2)]\n",
    "                # er_corrmat = np.corrcoef(encode_all, retrieve_all)[len(encode_all):, :len(retrieve_all)]\n",
    "                # er_corrmat_semipartial = np.corrcoef(encode_residual_all, retrieve_all)[len(encode_all):, :len(retrieve_all)]\n",
    "                # e_r_semipartial[hem].append(er_corrmat_semipartial)\n",
    "                \n",
    "                l_e_corr[hem].append(le_corrmat)\n",
    "                l_r_corr[hem].append(lr_corrmat)\n",
    "                i_e_corr[hem].append(ie_corrmat)\n",
    "                i_r_corr[hem].append(ir_corrmat)\n",
    "                l_l_corr[hem].append(ll_corrmat)\n",
    "                i_i_corr[hem].append(ii_corrmat)\n",
    "                \n",
    "        # save the weights and correlation matrices for the current subject and session\n",
    "        np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_shuffled_lh.txt',weight_store_by_sl_shuffled['lh'])\n",
    "        np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_shuffled_rh.txt',weight_store_by_sl_shuffled['rh'])   \n",
    "\n",
    "        np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_z_lh.txt',weight_store_by_sl_z['lh'])\n",
    "        np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_z_rh.txt',weight_store_by_sl_z['rh']) \n",
    "\n",
    "        np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_lh.txt',weight_store_by_sl['lh'])\n",
    "        np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_rh.txt',weight_store_by_sl['rh'])\n",
    "        \n",
    "        np.save(f'../outputs/corrmats/item_encoding/{sub}_{ses}_SL_lh.npy',np.array(i_e_corr['lh']))\n",
    "        np.save(f'../outputs/corrmats/item_encoding/{sub}_{ses}_SL_rh.npy',np.array(i_e_corr['rh']))\n",
    "        np.save(f'../outputs/corrmats/item_retrieval/{sub}_{ses}_SL_lh.npy',np.array(i_r_corr['lh']))\n",
    "        np.save(f'../outputs/corrmats/item_retrieval/{sub}_{ses}_SL_rh.npy',np.array(i_r_corr['rh']))\n",
    "        np.save(f'../outputs/corrmats/locus_encoding/{sub}_{ses}_SL_lh.npy',np.array(l_e_corr['lh']))\n",
    "        np.save(f'../outputs/corrmats/locus_encoding/{sub}_{ses}_SL_rh.npy',np.array(l_e_corr['rh']))\n",
    "        np.save(f'../outputs/corrmats/locus_retrieval/{sub}_{ses}_SL_lh.npy',np.array(l_r_corr['lh']))\n",
    "        np.save(f'../outputs/corrmats/locus_retrieval/{sub}_{ses}_SL_rh.npy',np.array(l_r_corr['rh']))\n",
    "        np.save(f'../outputs/corrmats/locus_locus/{sub}_{ses}_SL_lh.npy',np.array(l_l_corr['lh']))\n",
    "        np.save(f'../outputs/corrmats/locus_locus/{sub}_{ses}_SL_rh.npy',np.array(l_l_corr['rh']))\n",
    "        np.save(f'../outputs/corrmats/item_item/{sub}_{ses}_SL_lh.npy',np.array(i_i_corr['lh']))\n",
    "        np.save(f'../outputs/corrmats/item_item/{sub}_{ses}_SL_rh.npy',np.array(i_i_corr['rh']))\n",
    "        # np.save(f'../outputs/corrmats/encoding_retrieval_residuals/{sub}_{ses}_semipartial_SL_lh.npy',np.array(e_r_semipartial['lh']))\n",
    "        # np.save(f'../outputs/corrmats/encoding_retrieval_residuals/{sub}_{ses}_semipartial_SL_rh.npy',np.array(e_r_semipartial['rh']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Parallel(n_jobs=13)(delayed(get_sub_corr_SL)(sub) for sub in subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data for the z score of the weights, get a per subject mean across sessions, and convert to vortex. \n",
    "for sub in subjects:\n",
    "    weight_store_l = []\n",
    "    weight_store_r = []\n",
    "\n",
    "    for ses in ['ses-01','ses-02','ses-03']:\n",
    "        weight_store_l.append(np.loadtxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_z_lh.txt'))\n",
    "        weight_store_r.append(np.loadtxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_z_rh.txt'))\n",
    "\n",
    "    sub_weight_l = np.mean(np.array(weight_store_l),axis=0)\n",
    "    sub_weight_r = np.mean(np.array(weight_store_r),axis=0)\n",
    "    # extract mean locus, item, # and encoding weights for the left and right hemisphere\n",
    "    L, I, E = {},{},{}\n",
    "    L['L'] = sub_weight_l[:,0]\n",
    "    L['R'] = sub_weight_r[:,0]\n",
    "    I['L'] = sub_weight_l[:,1]\n",
    "    I['R'] = sub_weight_r[:,1]\n",
    "    E['L'] = sub_weight_l[:,2]\n",
    "    E['R'] = sub_weight_r[:,2]\n",
    "    # convert the weights from SL to voxel space, and save them in the outputs folder\n",
    "    locus_vox_l = SLtoVox(L, SLlist,nv)['L']\n",
    "    \n",
    "    locus_vox_r = SLtoVox(L, SLlist,nv)['R']\n",
    "    item_vox_l = SLtoVox(I, SLlist,nv)['L']\n",
    "    item_vox_r = SLtoVox(I, SLlist,nv)['R']\n",
    "    encode_vox_l = SLtoVox(E, SLlist,nv)['L']\n",
    "    encode_vox_r = SLtoVox(E, SLlist,nv)['R']\n",
    "    \n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_locus_weights_z_lh.txt',locus_vox_l)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_locus_weights_z_rh.txt',locus_vox_r)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_item_weights_z_lh.txt',item_vox_l)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_item_weights_z_rh.txt',item_vox_r)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_encode_weights_z_lh.txt',encode_vox_l)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_encode_weights_z_rh.txt',encode_vox_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data for raw weights, get a per subject mean across sessions, and convert to vortex. \n",
    "for sub in subjects:\n",
    "    weight_store_l = []\n",
    "    weight_store_r = []\n",
    "\n",
    "    for ses in ['ses-01','ses-02','ses-03']:\n",
    "        weight_store_l.append(np.loadtxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_lh.txt'))\n",
    "        weight_store_r.append(np.loadtxt(f'../outputs/regression_weights_predict_retrieval/{sub}_{ses}_enc_ret_par_corr_rh.txt'))\n",
    "\n",
    "    sub_weight_l = np.mean(np.array(weight_store_l),axis=0)\n",
    "    sub_weight_r = np.mean(np.array(weight_store_r),axis=0)\n",
    "    L, I, E = {},{},{}\n",
    "    L['L'] = sub_weight_l[:,0]\n",
    "    L['R'] = sub_weight_r[:,0]\n",
    "    I['L'] = sub_weight_l[:,1]\n",
    "    I['R'] = sub_weight_r[:,1]\n",
    "    E['L'] = sub_weight_l[:,2]\n",
    "    E['R'] = sub_weight_r[:,2]\n",
    "    locus_vox_l = SLtoVox(L, SLlist,nv)['L']\n",
    "    locus_vox_r = SLtoVox(L, SLlist,nv)['R']\n",
    "    item_vox_l = SLtoVox(I, SLlist,nv)['L']\n",
    "    item_vox_r = SLtoVox(I, SLlist,nv)['R']\n",
    "    encode_vox_l = SLtoVox(E, SLlist,nv)['L']\n",
    "    encode_vox_r = SLtoVox(E, SLlist,nv)['R']\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_locus_weights_lh.txt',locus_vox_l)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_locus_weights_rh.txt',locus_vox_r)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_item_weights_lh.txt',item_vox_l)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_item_weights_rh.txt',item_vox_r)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_encode_weights_lh.txt',encode_vox_l)\n",
    "    np.savetxt(f'../outputs/regression_weights_predict_retrieval/{sub}_vox_encode_weights_rh.txt',encode_vox_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subject in each session\n",
    "# compute the difference between diagonal and off diagonal of the correlation matrices between\n",
    "# locus-locus, item-item, item-encoding, locus-encoding \n",
    "# Get a z-score of difference between diag - off-diag for real data, and for permutated data\n",
    "# and convert into vertex maps\n",
    "def process_corrmats(sub):\n",
    "    for ses in ['ses-01','ses-02','ses-03']:\n",
    "        ie_corrmats_l = np.load(f'../outputs/corrmats/item_encoding/{sub}_{ses}_SL_lh.npy')\n",
    "        ie_corrmats_r = np.load(f'../outputs/corrmats/item_encoding/{sub}_{ses}_SL_rh.npy')\n",
    "        ie_corrmats_concate = np.concatenate([ie_corrmats_l,ie_corrmats_r])\n",
    "        # generate true (diagonal - offdiagonal) value of the correlation matrix, \n",
    "        # and a null distribution with (diagonal - offdiagonal) shuffled correlation matrix \n",
    "        perm_diag_diff_maps = get_diag_diff_perm(corrmat=ie_corrmats_concate, nPerm=1000)\n",
    "        # compute a z-score of the difference between diagonal and off-diagonal for the real data vs. the shuffled data\n",
    "        # convert it into vertex maps, and save it.\n",
    "        # this analysis is the same for all the subsequent correlation matrices\n",
    "        util.SL_array_to_maps(perm_diag_diff_maps, f'../outputs/brain_maps/{sub}_{ses}_item_encoding_SL_z',nSL_L, SLlist, nv)\n",
    "        \n",
    "        ie_diag_mean_l = np.mean(np.array([np.diag(matrix) for matrix in ie_corrmats_l]), axis=1)\n",
    "        ie_diag_mean_r = np.mean(np.array([np.diag(matrix) for matrix in ie_corrmats_r]), axis=1)\n",
    "\n",
    "        ie_diag_mean_vox = SLtoVox({'L':ie_diag_mean_l,'R':ie_diag_mean_r},SLlist,nv)\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_item_encoding_diag_mean_lh.txt',ie_diag_mean_vox['L'])\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_item_encoding_diag_mean_rh.txt',ie_diag_mean_vox['R'])\n",
    "\n",
    "        le_corrmats_l = np.load(f'../outputs/corrmats/locus_encoding/{sub}_{ses}_SL_lh.npy')\n",
    "        le_corrmats_r = np.load(f'../outputs/corrmats/locus_encoding/{sub}_{ses}_SL_rh.npy')\n",
    "        le_corrmats_concate = np.concatenate([le_corrmats_l,le_corrmats_r])\n",
    "        perm_diag_diff_maps = get_diag_diff_perm(corrmat=le_corrmats_concate, nPerm=1000)\n",
    "        util.SL_array_to_maps(perm_diag_diff_maps, f'../outputs/brain_maps/{sub}_{ses}_locus_encoding_SL_z',nSL_L, SLlist, nv)\n",
    "\n",
    "        le_diag_mean_l = np.mean(np.array([np.diag(matrix) for matrix in le_corrmats_l]), axis=1)\n",
    "        le_diag_mean_r = np.mean(np.array([np.diag(matrix) for matrix in le_corrmats_r]), axis=1)\n",
    "\n",
    "        le_diag_mean_vox = SLtoVox({'L':le_diag_mean_l,'R':le_diag_mean_r},SLlist,nv)\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_locus_encoding_diag_mean_lh.txt',le_diag_mean_vox['L'])\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_locus_encoding_diag_mean_rh.txt',le_diag_mean_vox['R'])\n",
    "\n",
    "        ll_corrmats_l = np.load(f'../outputs/corrmats/locus_locus/{sub}_{ses}_SL_lh.npy')\n",
    "        ll_corrmats_r = np.load(f'../outputs/corrmats/locus_locus/{sub}_{ses}_SL_rh.npy')\n",
    "        ll_corrmats_concate = np.concatenate([ll_corrmats_l,ll_corrmats_r])\n",
    "        perm_diag_diff_maps = get_diag_diff_perm(corrmat=ll_corrmats_concate, nPerm=1000)\n",
    "        util.SL_array_to_maps(perm_diag_diff_maps, f'../outputs/brain_maps/{sub}_{ses}_locus_locus_SL_z',nSL_L, SLlist, nv)\n",
    "\n",
    "        ll_diag_mean_l = np.mean(np.array([np.diag(matrix) for matrix in ll_corrmats_l]), axis=1)\n",
    "        ll_diag_mean_r = np.mean(np.array([np.diag(matrix) for matrix in ll_corrmats_r]), axis=1)\n",
    "\n",
    "        ll_diag_mean_vox = SLtoVox({'L':ll_diag_mean_l,'R':ll_diag_mean_r},SLlist,nv)\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_locus_locus_diag_mean_lh.txt',ll_diag_mean_vox['L'])\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_locus_locus_diag_mean_rh.txt',ll_diag_mean_vox['R'])\n",
    "\n",
    "\n",
    "        ii_corrmats_l = np.load(f'../outputs/corrmats/item_item/{sub}_{ses}_SL_lh.npy')\n",
    "        ii_corrmats_r = np.load(f'../outputs/corrmats/item_item/{sub}_{ses}_SL_rh.npy')\n",
    "        ii_corrmats_concate = np.concatenate([ii_corrmats_l,ii_corrmats_r])\n",
    "        perm_diag_diff_maps = get_diag_diff_perm(corrmat=ii_corrmats_concate, nPerm=1000)\n",
    "        util.SL_array_to_maps(perm_diag_diff_maps, f'../outputs/brain_maps/{sub}_{ses}_item_item_SL_z',nSL_L, SLlist, nv)\n",
    "\n",
    "        ii_diag_mean_l = np.mean(np.array([np.diag(matrix) for matrix in ii_corrmats_l]), axis=1)\n",
    "        ii_diag_mean_r = np.mean(np.array([np.diag(matrix) for matrix in ii_corrmats_r]), axis=1)\n",
    "\n",
    "        ii_diag_mean_vox = SLtoVox({'L':ii_diag_mean_l,'R':ii_diag_mean_r},SLlist,nv)\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_item_item_diag_mean_lh.txt',ii_diag_mean_vox['L'])\n",
    "        np.savetxt(f'../outputs/brain_maps/{sub}_{ses}_item_item_diag_mean_rh.txt',ii_diag_mean_vox['R'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13867891 0.4388947  0.07444446 ... 0.02855295 0.0310866  0.04240775]\n",
      "[0.38603704 0.20613178 0.16024064 ... 0.01647424 0.0196219  0.02074948]\n",
      "[0.49249731 0.00525682 0.02540761 ... 0.09148278 0.10338712 0.12434796]\n",
      "[0.11756232 0.38541199 0.36388231 ... 0.42574992 0.39438516 0.37035936]\n",
      "[0.01962757 0.36110768 0.10398444 ... 0.34128727 0.34652508 0.37368431]\n",
      "[0.2125592  0.21100839 0.45887494 ... 0.09095453 0.07756802 0.06533166]\n",
      "[0.25612088 0.09578015 0.23639844 ... 0.10351693 0.10464535 0.10448302]\n",
      "[0.17266921 0.2207868  0.32567664 ... 0.4719365  0.4812872  0.48813009]\n",
      "[0.18232421 0.38324154 0.33095486 ... 0.14871993 0.1402674  0.12479885]\n",
      "[0.46036107 0.46882978 0.2620461  ... 0.37691398 0.37653086 0.35225653]\n",
      "[0.14287156 0.38599657 0.25934312 ... 0.078234   0.06493888 0.04175953]\n",
      "[0.24575394 0.03969549 0.09048875 ... 0.11016197 0.1175785  0.11583619]\n",
      "[0.47783587 0.01076545 0.00385277 ... 0.27366749 0.32901234 0.38702611]\n",
      "[0.31388524 0.35967374 0.39688313 ... 0.09029291 0.09713435 0.09408531]\n",
      "[0.06319839 0.00055014 0.05680588 ... 0.48015227 0.4595853  0.46484907]\n",
      "[0.25285526 0.00876265 0.14522369 ... 0.37166923 0.35634071 0.28657683]\n",
      "[0.14897953 0.00063108 0.40221173 ... 0.3170015  0.29594886 0.29370821]\n",
      "[0.2682056  0.16074242 0.02385774 ... 0.41503999 0.42859322 0.46188409]\n",
      "[0.29943219 0.10528218 0.49931179 ... 0.46388593 0.45639987 0.46319094]\n",
      "[0.0061399  0.19802406 0.01692906 ... 0.10930855 0.1234519  0.13608025]\n",
      "[0.2009622  0.00052005 0.19101868 ... 0.38588288 0.43928945 0.47350815]\n",
      "[0.34050586 0.09385497 0.44375256 ... 0.14225659 0.13502694 0.13285937]\n",
      "[0.06491881 0.1107285  0.1650607  ... 0.39905918 0.45642231 0.4644842 ]\n",
      "[0.44913102 0.39051526 0.37597319 ... 0.45263375 0.42729952 0.32722204]\n",
      "[0.26818004 0.09612397 0.17102035 ... 0.16443244 0.18725736 0.18563527]\n",
      "[0.29213018 0.00104837 0.42913761 ... 0.29983673 0.27663636 0.25929134]\n",
      "[0.27531636 0.4261819  0.02378441 ... 0.02794515 0.02764227 0.01891896]\n",
      "[0.18736961 0.06207884 0.30959457 ... 0.19538347 0.1759574  0.1536366 ]\n",
      "[0.0710918  0.03955731 0.12737201 ... 0.4572395  0.451271   0.48160452]\n",
      "[0.20798171 0.11297763 0.15713781 ... 0.19573692 0.18613687 0.17412009]\n",
      "[0.02917184 0.19346691 0.24393046 ... 0.2810348  0.29151229 0.30681013]\n",
      "[0.20800987 0.25997163 0.37649571 ... 0.01232688 0.01508218 0.01838838]\n",
      "[0.11912157 0.00263635 0.01377642 ... 0.17203652 0.17018274 0.14729439]\n",
      "[0.28822993 0.26112424 0.36782418 ... 0.00950592 0.01162243 0.01458946]\n",
      "[0.42674836 0.01483488 0.15711965 ... 0.49059606 0.49015896 0.46633421]\n",
      "[0.39889016 0.01164906 0.16566963 ... 0.1868664  0.17664577 0.14822303]\n",
      "[0.27860538 0.0725794  0.48746329 ... 0.33550842 0.33215027 0.35673392]\n",
      "[1.54783771e-01 1.84503609e-05 4.47039198e-01 ... 2.54463833e-03\n",
      " 3.05551634e-03 3.10882024e-03]\n",
      "[0.42837241 0.06142242 0.09715456 ... 0.11335539 0.11638702 0.14565906]\n",
      "[0.04722325 0.00402549 0.14291688 ... 0.0818532  0.07719218 0.06324027]\n",
      "[0.26938328 0.28659741 0.06264939 ... 0.23947125 0.23091946 0.234351  ]\n",
      "[0.16592877 0.05007583 0.08312539 ... 0.43281054 0.43542919 0.45705912]\n",
      "[0.33177311 0.20645979 0.02086924 ... 0.01179197 0.01007949 0.00721234]\n",
      "[0.47989894 0.20521086 0.37812053 ... 0.3114724  0.32020355 0.33297131]\n",
      "[0.41083517 0.11223227 0.15386595 ... 0.07085897 0.07816044 0.10101734]\n",
      "[0.49073189 0.46014061 0.30932157 ... 0.00459357 0.00496297 0.00724504]\n",
      "[0.43648277 0.07597515 0.14154599 ... 0.4289673  0.38323157 0.33802158]\n",
      "[0.34740231 0.00236101 0.27629646 ... 0.32636133 0.34079564 0.44556331]\n",
      "[0.35242664 0.36948572 0.42551869 ... 0.23536154 0.21081392 0.23161394]\n",
      "[0.20044361 0.09045797 0.35886073 ... 0.34935381 0.34793362 0.35067972]\n",
      "[0.46056321 0.30033119 0.27371989 ... 0.42761602 0.4101173  0.35528567]\n",
      "[0.4897801  0.41689189 0.25622566 ... 0.35516368 0.32268144 0.26803451]\n",
      "[0.02030501 0.43144118 0.44721114 ... 0.36402825 0.37903534 0.37388322]\n",
      "[0.05156273 0.13900538 0.07613007 ... 0.30680592 0.29364155 0.26428643]\n",
      "[0.1306496  0.00442553 0.03491999 ... 0.02826316 0.02790283 0.02827186]\n",
      "[0.29170947 0.07610369 0.12709793 ... 0.39861229 0.37952018 0.36152449]\n",
      "[0.12835045 0.07147084 0.44757599 ... 0.47903319 0.49715152 0.49303536]\n",
      "[0.48038336 0.44727791 0.35556362 ... 0.23835442 0.25181272 0.28438889]\n",
      "[0.40798575 0.01741096 0.46442675 ... 0.37677194 0.39753874 0.43203878]\n",
      "[0.2420704  0.37280928 0.07230894 ... 0.10791448 0.10907415 0.11609668]\n",
      "[0.04411766 0.11433858 0.00276439 ... 0.18657512 0.21648289 0.26313424]\n",
      "[0.02990886 0.20591672 0.08532579 ... 0.47697539 0.47028919 0.41486673]\n",
      "[0.36592537 0.28877258 0.21901574 ... 0.09404189 0.07904266 0.08365873]\n",
      "[0.33399251 0.11528301 0.24610239 ... 0.02305963 0.02189684 0.03135572]\n",
      "[0.21355199 0.36745017 0.49546463 ... 0.37917225 0.41548931 0.46914548]\n",
      "[0.23576837 0.04163362 0.18775584 ... 0.19055535 0.18546508 0.20916157]\n",
      "[0.0594877  0.0180796  0.28015229 ... 0.00194545 0.00244226 0.00409917]\n",
      "[0.24968825 0.00259426 0.22322627 ... 0.05436243 0.06019985 0.08273948]\n",
      "[0.45258278 0.00991358 0.48566571 ... 0.38567253 0.36959279 0.31885615]\n",
      "[0.09390524 0.29372076 0.40091561 ... 0.3664646  0.36329861 0.33092851]\n",
      "[0.39275162 0.29538547 0.181866   ... 0.21606937 0.21869796 0.22477382]\n",
      "[0.00244761 0.00141137 0.22718302 ... 0.33131819 0.33578715 0.31055553]\n",
      "[0.07118667 0.04831818 0.00773662 ... 0.46991184 0.4288651  0.34833115]\n",
      "[0.39218573 0.00920664 0.21376366 ... 0.26864706 0.26598828 0.27127791]\n",
      "[0.0554636  0.02128868 0.1929758  ... 0.49032031 0.46811417 0.42880325]\n",
      "[0.46543094 0.24315186 0.05200401 ... 0.03950373 0.04756956 0.05466075]\n",
      "[0.05480085 0.41654742 0.16715445 ... 0.17917954 0.17771731 0.19306268]\n",
      "[1.18831979e-01 3.10133815e-05 1.49534002e-01 ... 4.23823573e-01\n",
      " 4.41349086e-01 4.08364484e-01]\n",
      "[0.35811351 0.19338538 0.00111767 ... 0.47612668 0.44728135 0.43948305]\n",
      "[0.11067668 0.01660345 0.03164918 ... 0.04496931 0.03762045 0.03184016]\n",
      "[0.06385432 0.06373395 0.44862491 ... 0.23799538 0.24276009 0.25472088]\n",
      "[7.49026508e-05 4.62309887e-02 2.03269864e-01 ... 3.20873329e-01\n",
      " 2.96006313e-01 3.02737923e-01]\n",
      "[1.39114210e-02 1.56742294e-04 1.83177255e-01 ... 2.18516389e-01\n",
      " 2.39548913e-01 2.72168748e-01]\n",
      "[1.23468944e-01 6.63629050e-06 2.25188143e-01 ... 4.57649561e-02\n",
      " 5.67255384e-02 6.39830755e-02]\n",
      "[2.69268834e-01 2.50977791e-07 4.09065780e-02 ... 3.67727931e-04\n",
      " 2.53132089e-04 1.15145111e-04]\n",
      "[0.21643012 0.02255488 0.04109639 ... 0.1759141  0.16702235 0.15927676]\n",
      "[3.06091219e-01 2.57654395e-04 3.03275110e-01 ... 2.34009469e-04\n",
      " 2.24373809e-04 2.37161880e-04]\n",
      "[0.1907683  0.08591474 0.41682636 ... 0.09603903 0.08474774 0.07692465]\n",
      "[0.46464526 0.15911127 0.40471542 ... 0.06822838 0.06840468 0.05145106]\n",
      "[0.02576342 0.00345336 0.46840788 ... 0.48879797 0.48570599 0.49769264]\n",
      "[0.25846549 0.12475067 0.27837317 ... 0.42568218 0.36716941 0.25862983]\n",
      "[0.24993387 0.17438828 0.45041387 ... 0.3977461  0.35158616 0.26043066]\n",
      "[0.35783308 0.44338762 0.1709673  ... 0.04560478 0.04647664 0.05200652]\n",
      "[0.15707166 0.00113081 0.00093565 ... 0.0128909  0.00974712 0.00720965]\n",
      "[0.21292291 0.14795645 0.18942286 ... 0.19712563 0.18776535 0.21146182]\n",
      "[0.18472536 0.36669036 0.29612426 ... 0.36225457 0.31135196 0.26271122]\n",
      "[0.27417557 0.42109018 0.15189136 ... 0.27617601 0.28713113 0.292448  ]\n",
      "[0.1101145  0.19478189 0.40712344 ... 0.3083142  0.28934168 0.29976612]\n",
      "[0.1228004  0.07352867 0.13293    ... 0.31649832 0.34382271 0.42805685]\n",
      "[0.12992671 0.09216272 0.29213282 ... 0.09826878 0.10625267 0.12570108]\n",
      "[3.35638071e-01 1.79933425e-07 3.13234743e-02 ... 7.01307897e-02\n",
      " 6.98473761e-02 5.55255872e-02]\n",
      "[0.43738351 0.40729287 0.21422639 ... 0.26245561 0.24208623 0.20881525]\n",
      "[0.31595876 0.46088879 0.25357494 ... 0.28508561 0.27663516 0.25269763]\n",
      "[0.06467864 0.0431473  0.0827439  ... 0.47267167 0.44942965 0.40781863]\n",
      "[0.38944184 0.08592    0.32608369 ... 0.20280742 0.2116156  0.23477641]\n",
      "[0.17814589 0.38768482 0.29026547 ... 0.42025835 0.43459727 0.42914964]\n",
      "[0.07446185 0.12709548 0.34812186 ... 0.29501276 0.29513714 0.30768734]\n",
      "[0.43642536 0.35997576 0.03369365 ... 0.0186056  0.02293223 0.02984127]\n",
      "[0.23962294 0.04795343 0.08026884 ... 0.04363326 0.04648152 0.05006884]\n",
      "[0.00902343 0.05035375 0.36284815 ... 0.31414737 0.29166853 0.26721128]\n",
      "[0.11827114 0.07787195 0.3265984  ... 0.15517219 0.17412793 0.21984813]\n",
      "[0.45333144 0.20923932 0.0702399  ... 0.33131866 0.34170175 0.33189262]\n",
      "[0.32423039 0.02485438 0.0600713  ... 0.27932786 0.24079292 0.22208954]\n",
      "[0.30929885 0.31540671 0.12904826 ... 0.09643213 0.09134913 0.09544106]\n",
      "[0.17491532 0.0177707  0.27672822 ... 0.46308174 0.45727378 0.42198445]\n",
      "[0.37450297 0.29955627 0.3465809  ... 0.36469235 0.36106629 0.35171123]\n",
      "[3.46635095e-01 3.70989014e-01 1.27757105e-05 ... 2.57438480e-01\n",
      " 2.41723630e-01 2.26959495e-01]\n",
      "[0.28368484 0.2629822  0.05227797 ... 0.41842569 0.43451264 0.43434724]\n",
      "[0.27285881 0.47047213 0.27527717 ... 0.3475887  0.37244996 0.33767667]\n",
      "[0.35109111 0.05536513 0.30699672 ... 0.04087626 0.03295122 0.02611005]\n",
      "[0.40551992 0.35143395 0.38697071 ... 0.05337758 0.05372347 0.05221893]\n",
      "[0.27607136 0.4782166  0.12293702 ... 0.11626446 0.12747211 0.1288924 ]\n",
      "[0.05135816 0.27119173 0.29006218 ... 0.42595518 0.44021284 0.4164763 ]\n",
      "[0.22482729 0.11138948 0.29971965 ... 0.12042035 0.11297488 0.09556097]\n",
      "[0.33122655 0.34615363 0.3011568  ... 0.47518704 0.47904018 0.4516487 ]\n",
      "[0.3934765  0.00230162 0.48597998 ... 0.31518959 0.31967695 0.31210128]\n",
      "[0.29587012 0.01691308 0.28629469 ... 0.04663697 0.04487447 0.0523768 ]\n",
      "[0.22247211 0.03860413 0.32301149 ... 0.03638037 0.02976857 0.02674507]\n",
      "[0.36179022 0.08693123 0.36340272 ... 0.14842885 0.16156285 0.18268119]\n",
      "[0.31909601 0.18859561 0.18526261 ... 0.37815466 0.37216941 0.34695811]\n",
      "[0.17135969 0.26899153 0.20894171 ... 0.02780964 0.02319087 0.0225758 ]\n",
      "[0.39758055 0.04636562 0.17939825 ... 0.02476597 0.02775173 0.04156259]\n",
      "[0.46497382 0.46117286 0.31352992 ... 0.34828559 0.34498685 0.33724257]\n",
      "[0.44474235 0.08108586 0.27944028 ... 0.13111983 0.11655697 0.09506731]\n",
      "[0.4275897  0.09447987 0.46732876 ... 0.42025085 0.41579379 0.38120072]\n",
      "[0.32359467 0.0038394  0.25685461 ... 0.19167947 0.16563166 0.12819028]\n",
      "[0.24837877 0.01399121 0.11481435 ... 0.07527499 0.07190263 0.07177829]\n",
      "[0.3800056  0.06315935 0.37954116 ... 0.36062922 0.32491541 0.34717966]\n",
      "[0.10345442 0.4144662  0.17349562 ... 0.32911738 0.30108863 0.25398163]\n",
      "[0.13054571 0.26613538 0.02914148 ... 0.4721683  0.49896288 0.4674316 ]\n",
      "[0.38751988 0.05721966 0.37078908 ... 0.34912342 0.36920272 0.34401712]\n",
      "[0.28890961 0.01989473 0.02895075 ... 0.27139735 0.27903262 0.28386955]\n",
      "[0.392671   0.36484625 0.38817035 ... 0.06749445 0.06480621 0.05787079]\n",
      "[0.0284093  0.05440453 0.08082475 ... 0.17286482 0.17561727 0.18141335]\n",
      "[0.03563666 0.47529097 0.28393161 ... 0.17497958 0.19017694 0.19691863]\n",
      "[0.38926197 0.10708934 0.38125209 ... 0.41527255 0.41731936 0.40683004]\n",
      "[0.49323436 0.21923938 0.20564364 ... 0.44478386 0.42759768 0.48018786]\n",
      "[0.38520903 0.06866167 0.22955911 ... 0.25329425 0.20731775 0.18064817]\n",
      "[0.33181912 0.38775412 0.11083278 ... 0.32039357 0.33859015 0.38486158]\n",
      "[0.42291495 0.00556038 0.47656469 ... 0.19810395 0.19729907 0.22362151]\n",
      "[0.23123597 0.05264134 0.4169505  ... 0.41063365 0.405065   0.39780123]\n",
      "[0.35040106 0.27200224 0.1125901  ... 0.22843245 0.21412164 0.21790086]\n",
      "[0.08304589 0.21362093 0.05602984 ... 0.04254438 0.04126599 0.0519401 ]\n",
      "[0.48061975 0.03304899 0.45329462 ... 0.08202842 0.08070439 0.10497889]\n",
      "[0.35328407 0.06025125 0.36224528 ... 0.13470252 0.14607562 0.154326  ]\n",
      "[0.22524255 0.28595353 0.09629852 ... 0.32036939 0.33370868 0.31610968]\n",
      "[0.18957882 0.00548107 0.41763392 ... 0.36122505 0.39455679 0.43746613]\n",
      "[0.24495303 0.33409597 0.07952976 ... 0.05266266 0.05114705 0.05483624]\n",
      "[0.22175173 0.09948309 0.29542461 ... 0.49829775 0.47350692 0.47282023]\n",
      "[0.19128091 0.46029007 0.19313889 ... 0.35130229 0.30582237 0.30959778]\n",
      "[0.11141973 0.18469811 0.12248072 ... 0.1782967  0.20225457 0.22334919]\n",
      "[0.16985681 0.38033326 0.07022877 ... 0.11345351 0.1083133  0.11198821]\n",
      "[0.180712   0.22678137 0.33687026 ... 0.38707928 0.37742369 0.35991346]\n",
      "[0.35297602 0.29609991 0.29284455 ... 0.15182118 0.17071104 0.20760081]\n",
      "[0.38659627 0.30074151 0.4290283  ... 0.00819819 0.00794078 0.01072544]\n",
      "[0.07882151 0.02830693 0.10791635 ... 0.05169529 0.04539192 0.03239753]\n",
      "[0.11050483 0.49218847 0.07284877 ... 0.09723397 0.09968703 0.09429506]\n",
      "[0.44452364 0.08479807 0.20230445 ... 0.47143752 0.48834006 0.45180122]\n",
      "[0.40151902 0.04044136 0.14953684 ... 0.08069753 0.06919267 0.06328667]\n",
      "[0.08198631 0.47927984 0.06740442 ... 0.13915665 0.12031551 0.12395931]\n",
      "[0.10306494 0.36815565 0.24703031 ... 0.22996432 0.20194024 0.1382861 ]\n",
      "[0.23128986 0.4129267  0.09582124 ... 0.30613597 0.28966491 0.19984625]\n",
      "[0.15712339 0.41253533 0.02481848 ... 0.10258836 0.10708549 0.11565531]\n",
      "[0.25269375 0.11535371 0.13228939 ... 0.05494508 0.05849658 0.05871344]\n",
      "[0.31196702 0.03666729 0.46057165 ... 0.20554448 0.18873069 0.18441033]\n",
      "[3.13750271e-01 9.34773229e-05 4.31406089e-01 ... 2.22661348e-01\n",
      " 2.32201176e-01 2.79987132e-01]\n",
      "[0.05498688 0.01171781 0.13684725 ... 0.02440204 0.02203448 0.02634075]\n",
      "[0.45296447 0.02782934 0.35448254 ... 0.1583857  0.15833137 0.1376274 ]\n",
      "[0.06866107 0.00897243 0.05176846 ... 0.35075344 0.34520599 0.33446282]\n",
      "[8.31387153e-02 6.57049260e-06 3.12675513e-01 ... 6.52779964e-02\n",
      " 5.18236810e-02 4.10048745e-02]\n",
      "[0.3606953  0.23988196 0.22553062 ... 0.33849254 0.33822298 0.31391923]\n",
      "[0.29013338 0.01373326 0.00804619 ... 0.33041684 0.36257126 0.45653959]\n",
      "[0.34754718 0.40092672 0.0107683  ... 0.22252814 0.20501822 0.18099861]\n",
      "[0.43272103 0.12564304 0.34613208 ... 0.33331299 0.34271397 0.37505574]\n",
      "[1.52181652e-05 3.97385868e-03 8.81780848e-02 ... 2.51156519e-01\n",
      " 2.77822140e-01 2.87830050e-01]\n",
      "[0.06591363 0.23315505 0.15839322 ... 0.01832473 0.02201209 0.02758491]\n",
      "[0.0119378  0.27342319 0.11764929 ... 0.24841565 0.20835021 0.18923098]\n",
      "[0.12907095 0.26807545 0.26905798 ... 0.42084316 0.43687306 0.44833511]\n",
      "[0.48243915 0.0077901  0.28248261 ... 0.12541717 0.12123646 0.12564652]\n",
      "[0.42912819 0.38158947 0.35959633 ... 0.25593666 0.24821471 0.24615708]\n",
      "[3.36824911e-01 2.72031360e-01 3.98181892e-01 ... 1.17163072e-04\n",
      " 5.92413311e-05 3.29060897e-05]\n",
      "[0.27287041 0.22547607 0.22986986 ... 0.44770933 0.46019463 0.47253815]\n",
      "[0.37325099 0.39820344 0.33288911 ... 0.27758673 0.29912658 0.33053318]\n",
      "[0.31746265 0.48245463 0.05104861 ... 0.15619302 0.14514763 0.11483855]\n",
      "[0.18204137 0.49691739 0.29353951 ... 0.20593988 0.21836536 0.21598947]\n",
      "[0.22671234 0.41767334 0.42852793 ... 0.4916879  0.48652685 0.44157572]\n",
      "[0.2807454  0.11506179 0.44726864 ... 0.10073878 0.10072123 0.11324549]\n",
      "[0.33068059 0.34234234 0.16267322 ... 0.13070117 0.13120384 0.12321809]\n",
      "[0.00808775 0.36921429 0.4422067  ... 0.09119234 0.10307429 0.11866921]\n",
      "[0.25124459 0.0028102  0.26987406 ... 0.3848169  0.39737829 0.40510734]\n",
      "[2.02101373e-01 4.41463913e-05 6.70697606e-03 ... 2.58780113e-03\n",
      " 2.42627580e-03 2.25534312e-03]\n",
      "[0.34258521 0.1779909  0.45167736 ... 0.05780543 0.07026029 0.09043784]\n",
      "[0.29880505 0.4194736  0.19199947 ... 0.45116823 0.41259822 0.39344618]\n",
      "[0.13030885 0.14597022 0.12761582 ... 0.09665672 0.10912961 0.14880977]\n",
      "[0.35748717 0.26958543 0.45372432 ... 0.28448982 0.30094395 0.31187527]\n",
      "[0.28289992 0.09996288 0.16748708 ... 0.11995087 0.09538013 0.07468917]\n",
      "[0.45851558 0.24511259 0.10636501 ... 0.03060972 0.02806295 0.03940005]\n",
      "[0.36443039 0.1357207  0.45140087 ... 0.24170015 0.22676956 0.25160939]\n",
      "[0.06846082 0.00082374 0.05797696 ... 0.29852531 0.26839938 0.22832582]\n",
      "[0.35478504 0.00096359 0.49323818 ... 0.14239309 0.14096998 0.14313866]\n",
      "[0.08053477 0.42948124 0.03690572 ... 0.09905748 0.10334385 0.08874572]\n",
      "[0.13550803 0.0583245  0.14914423 ... 0.01838311 0.01324123 0.01077149]\n",
      "[0.02480902 0.48757256 0.00652075 ... 0.0534161  0.04663223 0.03984777]\n",
      "[0.46153469 0.45475365 0.49900246 ... 0.34943878 0.35092329 0.3614465 ]\n",
      "[0.08789792 0.37163724 0.45248166 ... 0.38214821 0.34148028 0.31139459]\n",
      "[0.05906927 0.06105461 0.00459325 ... 0.41727015 0.44192621 0.49718691]\n",
      "[0.31775286 0.47157069 0.10827208 ... 0.31156563 0.30432488 0.36603063]\n",
      "[0.03363441 0.30797257 0.48066571 ... 0.13136112 0.11442763 0.10484184]\n",
      "[0.02954479 0.33492462 0.21003383 ... 0.03982642 0.03362075 0.02388668]\n",
      "[0.21385989 0.33932702 0.36830787 ... 0.43547225 0.44988408 0.46079544]\n",
      "[0.1034304  0.37296986 0.24712261 ... 0.18716183 0.18323573 0.20349103]\n",
      "[0.26698807 0.04382587 0.24146302 ... 0.26722624 0.27664988 0.31007151]\n",
      "[0.3236737  0.36652417 0.14387228 ... 0.0752915  0.07620616 0.06178892]\n",
      "[0.25601886 0.14636252 0.29580344 ... 0.23874342 0.24086823 0.19988261]\n",
      "[0.03478042 0.14780765 0.17992291 ... 0.19275135 0.1980954  0.1705943 ]\n",
      "[0.46841858 0.03722203 0.12969378 ... 0.25131223 0.21914568 0.24111072]\n",
      "[0.19674483 0.00484978 0.37251847 ... 0.03952207 0.04209075 0.04984611]\n",
      "[0.00956141 0.18982657 0.39005072 ... 0.41442987 0.43105582 0.43518598]\n",
      "[0.00428561 0.00457233 0.15707869 ... 0.00109263 0.00103123 0.00135264]\n",
      "[4.08343057e-01 3.57773597e-06 3.09352237e-01 ... 1.97876674e-01\n",
      " 1.89360930e-01 1.74556117e-01]\n",
      "[0.42103797 0.00107214 0.46343612 ... 0.28592424 0.26351274 0.19459748]\n",
      "[0.31057065 0.11524539 0.44709339 ... 0.0484727  0.04376365 0.03842513]\n",
      "[0.15882949 0.20012563 0.01757786 ... 0.38132128 0.38994453 0.40645759]\n",
      "[0.40307076 0.37883426 0.22751184 ... 0.09656863 0.09506036 0.1056708 ]\n",
      "[0.20339731 0.04261195 0.13830057 ... 0.01571852 0.01457242 0.01042816]\n",
      "[0.09574787 0.03903154 0.10023434 ... 0.21855701 0.22420007 0.27028312]\n",
      "[0.38492597 0.206492   0.35899694 ... 0.33045274 0.33012155 0.35958971]\n",
      "[0.30834143 0.42112839 0.01724029 ... 0.49508032 0.49882431 0.4704526 ]\n",
      "[0.42612487 0.22506141 0.23484015 ... 0.00323384 0.00273136 0.0020277 ]\n",
      "[0.10951043 0.37057958 0.13676718 ... 0.3684039  0.3721474  0.37684317]\n",
      "[0.30989027 0.09165562 0.23214955 ... 0.41973331 0.42264526 0.40252256]\n",
      "[0.4759494  0.37482886 0.1676169  ... 0.05763952 0.05489702 0.03661329]\n",
      "[0.18593724 0.33291679 0.24748742 ... 0.32511476 0.3480516  0.40035872]\n",
      "[0.2941871  0.46826939 0.19987083 ... 0.11473291 0.11161308 0.12046403]\n",
      "[0.14059328 0.49159607 0.05070385 ... 0.15223538 0.1554299  0.157265  ]\n",
      "[0.16519062 0.09030692 0.0519604  ... 0.22044207 0.20297841 0.18218973]\n",
      "[0.12245963 0.17760317 0.02721259 ... 0.13266716 0.1036009  0.08654671]\n",
      "[0.4876874  0.4291906  0.46024398 ... 0.46645136 0.44989568 0.43540666]\n",
      "[5.69161338e-03 3.22099738e-05 4.14195483e-01 ... 9.50190738e-03\n",
      " 8.39193389e-03 9.05273436e-03]\n",
      "[0.4290094  0.43189892 0.22154452 ... 0.45730176 0.47513893 0.48523005]\n",
      "[0.47711008 0.0180839  0.15250445 ... 0.08642674 0.08284225 0.08617314]\n",
      "[0.30085651 0.2555272  0.00186852 ... 0.26143267 0.28478398 0.35800646]\n",
      "[0.33399689 0.30507207 0.36812366 ... 0.2508998  0.28724493 0.35192749]\n",
      "[0.17790166 0.32073886 0.25887228 ... 0.30761341 0.28900053 0.28851139]\n",
      "[0.45061542 0.18870576 0.24612624 ... 0.1253005  0.13325713 0.13681525]\n",
      "[0.36488381 0.23917376 0.08188423 ... 0.24478024 0.25003911 0.2516264 ]\n",
      "[0.27089409 0.15693468 0.41052849 ... 0.32608208 0.30068841 0.29094852]\n",
      "[0.30924926 0.18980231 0.07839513 ... 0.34088468 0.3125127  0.28967018]\n",
      "[0.18905921 0.09691663 0.21428245 ... 0.45107359 0.48774725 0.41565669]\n",
      "[0.29155532 0.04776594 0.48374843 ... 0.15671134 0.15844669 0.14055456]\n",
      "[0.054433   0.1136086  0.23516338 ... 0.05781034 0.06276024 0.06790109]\n",
      "[0.29752513 0.06882307 0.49517761 ... 0.10363184 0.0991907  0.0969253 ]\n",
      "[0.0012261  0.00082888 0.00513233 ... 0.01982264 0.02258668 0.03753559]\n",
      "[0.47145078 0.48283423 0.07031406 ... 0.21759932 0.23419595 0.2902371 ]\n",
      "[0.16029935 0.01407985 0.21334048 ... 0.0653015  0.06713408 0.10153723]\n",
      "[0.28661105 0.35134952 0.01872313 ... 0.48437743 0.47904994 0.47949365]\n",
      "[0.39385894 0.02645203 0.0162625  ... 0.26237226 0.25865539 0.23855267]\n",
      "[0.17762793 0.01580973 0.23504191 ... 0.02647794 0.02806742 0.02988215]\n",
      "[0.19489562 0.29733589 0.16066519 ... 0.48283205 0.47232569 0.41622195]\n",
      "[0.31200368 0.1957848  0.12086183 ... 0.26071145 0.29332052 0.281156  ]\n",
      "[0.13798576 0.00313876 0.25363846 ... 0.00546853 0.00456519 0.00587995]\n",
      "[0.26208681 0.3916538  0.39925304 ... 0.42977338 0.41966685 0.39656063]\n",
      "[0.2492876  0.19736869 0.04225898 ... 0.31278904 0.34141696 0.33230283]\n",
      "[0.28750892 0.03010322 0.25190683 ... 0.0026267  0.00314959 0.00491911]\n",
      "[0.01487587 0.05463727 0.16832506 ... 0.00696982 0.00532409 0.00429748]\n",
      "[0.29663259 0.00556928 0.24617174 ... 0.19921403 0.20270261 0.20874802]\n",
      "[1.30614918e-03 5.72818214e-05 1.68016347e-01 ... 8.61884543e-02\n",
      " 9.65451295e-02 1.03102245e-01]\n",
      "[0.29860209 0.14654265 0.23440701 ... 0.30628508 0.3257395  0.35958905]\n",
      "[0.115759   0.24070834 0.32104993 ... 0.14508872 0.13803935 0.13110636]\n",
      "[0.20615104 0.25993868 0.08685177 ... 0.02694767 0.02843273 0.03192181]\n",
      "[0.01447839 0.00095566 0.20401367 ... 0.00447123 0.0054742  0.0060902 ]\n",
      "[0.31996309 0.23774973 0.35977219 ... 0.34067458 0.3363711  0.29153016]\n",
      "[0.03536548 0.25706177 0.43502357 ... 0.02326244 0.02582761 0.03167424]\n",
      "[0.44138808 0.09754618 0.30126281 ... 0.3951518  0.39118857 0.37665284]\n",
      "[0.26927815 0.12420044 0.35339391 ... 0.19484819 0.23342656 0.3029898 ]\n",
      "[0.36233493 0.04354539 0.09088041 ... 0.19988882 0.21703683 0.26809881]\n",
      "[0.06715946 0.03313864 0.21559903 ... 0.32019767 0.30889028 0.29532914]\n",
      "[0.23573629 0.01963737 0.00149695 ... 0.31226515 0.3097769  0.31845419]\n",
      "[0.07514348 0.2318836  0.10506172 ... 0.12711203 0.15581918 0.20862523]\n",
      "[0.10681394 0.04432932 0.34395009 ... 0.33935791 0.33880177 0.31271997]\n",
      "[0.17258017 0.19490778 0.27713296 ... 0.45682192 0.46009685 0.42949702]\n",
      "[0.49333164 0.26552207 0.04136347 ... 0.17753323 0.17572254 0.23342319]\n",
      "[0.20634543 0.49785746 0.03884524 ... 0.23705585 0.22824459 0.26402303]\n",
      "[1.03053530e-01 2.17046716e-04 2.84310775e-01 ... 2.15136831e-01\n",
      " 2.00520482e-01 1.74476423e-01]\n",
      "[0.0524496  0.22442842 0.11155073 ... 0.11332573 0.1264023  0.14752305]\n",
      "[0.39515281 0.35546836 0.1322373  ... 0.03872405 0.03889485 0.05409532]\n",
      "[0.05065587 0.13107693 0.33301738 ... 0.02257459 0.02087416 0.02213757]\n",
      "[0.20100243 0.03168884 0.00272736 ... 0.39770067 0.38094486 0.35131404]\n",
      "[0.48582165 0.21274911 0.28342858 ... 0.31371156 0.29769394 0.30281811]\n",
      "[0.19860791 0.01374847 0.30896286 ... 0.2508503  0.23940211 0.2530807 ]\n"
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(13) as pool:\n",
    "        pool.map(process_corrmats, subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv Environment",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e910363b8740c721c5ce320633253382eaafbe24cfdf305a5e03a8a59a2f3983"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
