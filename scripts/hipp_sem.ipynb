{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import deepdish as dd\n",
    "import string\n",
    "try:\n",
    "    os.chdir('/data/MoL_clean/scripts')\n",
    "except:\n",
    "    pass\n",
    "import util\n",
    "# util has some variables in them\n",
    "# import GLM_helper as gh\n",
    "\n",
    "import scipy.stats as stats\n",
    "import glob\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.stats import ttest_1samp\n",
    "import seaborn as sns\n",
    "from scipy import stats, linalg\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "import random\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "\n",
    "\n",
    "def getSentenceSimilarity(sentence1, sentence2):\n",
    "    return model.predict([(sentence1, sentence2)])[0]\n",
    "\n",
    "sub2subj = {\"sub-01\":\"subj001\", \"sub-02\":\"subj002\",\"sub-03\":\"subj003\",\"sub-04\":\"subj005\",\n",
    "              \"sub-05\":\"subj006\", \"sub-06\":\"subj007\", \"sub-07\":\"subj008\", \"sub-08\":\"subj009\", \n",
    "           \"sub-09\":\"subj010\",\"sub-10\":\"subj011\",\"sub-11\":\"subj013\", \"sub-12\":\"subj014\", \n",
    "            \"sub-13\":\"subj017\", \"sub-14\":\"subj018\", \"sub-15\":\"subj019\", \"sub-16\":\"subj020\",\n",
    "           \"sub-17\":\"subj021\", \"sub-18\":\"subj022\", \"sub-19\":\"subj023\", \"sub-20\":\"subj024\",'sub-21':'subj025',\n",
    "           'sub-22':'subj026','sub-23':'subj027','sub-24':'subj029','sub-25':'subj031',\n",
    "            'sub-101':'subj101', 'sub-102':'subj102', 'sub-103':'subj103', 'sub-105':'subj105','sub-107':'subj107','sub-108':'subj108'}\n",
    "ses2w = {\"ses-01\":\"W2\", \"ses-02\":\"W4D1\", \"ses-03\":\"W4D2\"}\n",
    "\n",
    "\n",
    "nv = 40962\n",
    "\n",
    "subjects = ['sub-%.2d'%s for s in range(1,26)]\n",
    "sessions = ['ses-%.2d'%s for s in range(1,4)]\n",
    "runs = ['run-%.2d'%s for s in range(1,3)]\n",
    "TR = 1.5\n",
    "nTRs = {'Item':302, 'Loci':302, 'Encode':355}\n",
    "\n",
    "nTRs_w4d2 = {'Item': 156, 'Loci': 156, 'Encode': 182}\n",
    "SL_lh = list(dd.io.load('SLlist_verydense.lh.h5').values())\n",
    "SL_rh = list(dd.io.load('SLlist_verydense.rh.h5').values())\n",
    "ag = list(dd.io.load('ROIs/Ang_verts.h5').values())\n",
    "pmc = list(dd.io.load('ROIs/PMC_verts.h5').values())\n",
    "mPFC = list(dd.io.load('ROIs/mPFC_verts.h5').values())\n",
    "ROIs = {'ag':ag, 'pmc':pmc, 'mpfc':mPFC}\n",
    "SLlist = {'L':SL_lh, \"R\": SL_rh}\n",
    "nSL_L = len(SLlist['L'])\n",
    "hippo_ROIs = ['anterior_hipp','posterior_hipp','hippo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_xs(string_list):\n",
    "    return [s for s in string_list if s!='x']\n",
    "\n",
    "\n",
    "def add_numbers_to_duplicates(string_list):\n",
    "    count = {}\n",
    "    new_list = []\n",
    "    \n",
    "    for item in string_list:\n",
    "        if item in count and item!='x':\n",
    "            count[item] += 1\n",
    "            new_item = f\"{item} {count[item]}\"\n",
    "        else:\n",
    "            count[item] = 1\n",
    "            new_item = item\n",
    "        \n",
    "        new_list.append(new_item.lower())\n",
    "        \n",
    "    return new_list\n",
    "\n",
    "\n",
    "def partial_corr(C, desired_i= None, desired_j=None):\n",
    "    \"\"\"\n",
    "    Returns the sample linear partial correlation coefficients between pairs of variables in C, controlling \n",
    "    for the remaining variables in C.\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n, p)\n",
    "        Array with the different variables. Each column of C is taken as a variable\n",
    "    Desired_i, desired_j: int\n",
    "        If only wants to calculate the partial correlation between desired_i and desired_j, set them to be the index of the variables\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    P : array-like, shape (p, p)\n",
    "        P[i, j] contains the partial correlation of C[:, i] and C[:, j] controlling\n",
    "        for the remaining variables in C\n",
    "    \"\"\"\n",
    "    \n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p), dtype=np.float)\n",
    "    if desired_i is not None and desired_j is not None:\n",
    "        P_corr[desired_i, desired_j] = 1\n",
    "        P_corr[desired_j, desired_i] = 1\n",
    "        idx = np.ones(p, dtype=np.bool)\n",
    "        idx[desired_i] = False\n",
    "        idx[desired_j] = False\n",
    "        beta_i = linalg.lstsq(C[:, idx], C[:, desired_j])[0]\n",
    "        beta_j = linalg.lstsq(C[:, idx], C[:, desired_i])[0]\n",
    "\n",
    "        res_j = C[:, desired_j] - C[:, idx].dot( beta_i)\n",
    "        res_i = C[:, desired_i] - C[:, idx].dot(beta_j)\n",
    "\n",
    "        corr = stats.pearsonr(res_i, res_j)[0]\n",
    "        P_corr[desired_i, desired_j] = corr\n",
    "        P_corr[desired_j, desired_i] = corr\n",
    "        P_corr = corr\n",
    "    else:\n",
    "        for i in range(p):\n",
    "            P_corr[i, i] = 1\n",
    "            for j in range(i+1, p):\n",
    "                idx = np.ones(p, dtype=np.bool)\n",
    "                idx[i] = False\n",
    "                idx[j] = False\n",
    "                beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n",
    "                beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n",
    "\n",
    "                res_j = C[:, j] - C[:, idx].dot( beta_i)\n",
    "                res_i = C[:, i] - C[:, idx].dot(beta_j)\n",
    "                \n",
    "                corr = stats.pearsonr(res_i, res_j)[0]\n",
    "                P_corr[i, j] = corr\n",
    "                P_corr[j, i] = corr\n",
    "            \n",
    "    return P_corr\n",
    "\n",
    "def SLtoVox(D, SLlist, nv, zeronan=True):\n",
    "    # D is dict of L, R, with N x arbitrary dims\n",
    "    # SLlist is dict of L, R list of length N, with vertices for each SL\n",
    "\n",
    "    Dvox = dict()\n",
    "    Dcount = dict()\n",
    "    for hem in ['L', 'R']:\n",
    "        Dvox[hem] = np.zeros((nv,)+ D[hem].shape[1:])\n",
    "        Dcount[hem] = np.zeros((nv,)+(1,)*len(D[hem].shape[1:]))\n",
    "        for i in range(len(SLlist[hem])):\n",
    "            Dvox[hem][SLlist[hem][i]] += D[hem][i]\n",
    "            Dcount[hem][SLlist[hem][i]] += 1\n",
    "\n",
    "        Dcount[hem][Dcount[hem] == 0] = np.nan\n",
    "        Dvox[hem] = Dvox[hem] / Dcount[hem]\n",
    "\n",
    "        if zeronan:\n",
    "            Dvox[hem][np.isnan(Dvox[hem])] = 0\n",
    "\n",
    "    return Dvox\n",
    "\n",
    "    \n",
    "def get_beta_dicts(sub,ses,hippo=False, expert = False):\n",
    "    task = 'Item'\n",
    "    # load item \n",
    "    item_filenames = sorted(glob.glob(f'../behavioral/{sub2subj[sub]}/{ses2w[ses]}/*{task.lower()}*.csv'))\n",
    "    item_words_lists = [[w for w in pd.read_csv(item_filenames[0])['Word'] if w is not np.nan],[w for w in pd.read_csv(item_filenames[1])['Word'] if w is not np.nan]]\n",
    "    item_beta_dict = {'lh':{w:[] for w in item_words_lists[0]}, 'rh':{w:[] for w in item_words_lists[0]},'anterior_hipp':{w:[] for w in item_words_lists[0]},'posterior_hipp':{w:[] for w in item_words_lists[0]},'hippo':{w:[] for w in item_words_lists[0]}}\n",
    "    task = 'Loci'\n",
    "    # load loci lists, with loci names in two lists. Create a dictionary accordingly taking the union of the two lists\n",
    "    loci_lists = [add_numbers_to_duplicates(list(pd.read_excel('../updated_sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name='%sloci1'%ses2w[ses].lower())['spoken_loci'])),\n",
    "                        add_numbers_to_duplicates(list(pd.read_excel('../updated_sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name='%sloci2'%ses2w[ses].lower())['spoken_loci'])),]\n",
    "    loci_beta_dict = {\"lh\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))}, \"rh\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},\"anterior_hipp\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},\"posterior_hipp\":{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))},'hippo':{l:[] for l in list(set(loci_lists[0]+loci_lists[1]))}}\n",
    "    \n",
    "    # create dictionary of locus and item for each of the two runs\n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp','hippo']:\n",
    "        run = 'run-01'\n",
    "        loci_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, l in enumerate(loci_lists[0]):\n",
    "            loci_beta_dict[hem][l].append(loci_fmri[:,i])\n",
    "        run = 'run-02'\n",
    "        loci_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, l in enumerate(loci_lists[1]):\n",
    "            loci_beta_dict[hem][l].append(loci_fmri[:,i])\n",
    "    # average locus rep if spoken in both runs\n",
    "        for l in loci_beta_dict[hem]:\n",
    "            if len(loci_beta_dict[hem][l]) == 2:\n",
    "                loci_beta_dict[hem][l] = np.mean(loci_beta_dict[hem][l],axis=0)\n",
    "            else:\n",
    "                loci_beta_dict[hem][l] = loci_beta_dict[hem][l][0]\n",
    "    \n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp','hippo']: \n",
    "        run = 'run-01'\n",
    "        task = 'Item'\n",
    "        item_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, w in enumerate(item_words_lists[0]):\n",
    "            item_beta_dict[hem][w].append(item_fmri[:,i])\n",
    "        run = 'run-02'\n",
    "        item_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%(task.lower(),sub,ses,run,hem))\n",
    "        for i, w in enumerate(item_words_lists[1]):\n",
    "            item_beta_dict[hem][w].append(item_fmri[:,i])\n",
    "\n",
    "        for w in item_beta_dict[hem]:\n",
    "            item_beta_dict[hem][w] = np.mean(item_beta_dict[hem][w],axis=0)\n",
    "\n",
    "    retrieve_beta_dict = {'lh':{},'rh':{},'anterior_hipp':{},'posterior_hipp':{},'hippo':{}}\n",
    "    encode_beta_dict = {'lh':{},'rh':{},'anterior_hipp':{},'posterior_hipp':{},'hippo':{}}\n",
    "    run = 'run-01'\n",
    "    for hem in ['lh','rh','anterior_hipp','posterior_hipp','hippo']:\n",
    "        recall_sheet = pd.read_excel('../updated_sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name=ses2w[ses].lower())\n",
    "        recall_sheet['retrieval'] = recall_sheet['retrieval'].apply(lambda x: x.split(' ')[0])\n",
    "        recall_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%('retrieve',sub,ses,run,hem))\n",
    "        encode_fmri = np.loadtxt('../outputs/betas/%s/%s_%s_%s_%s_beta.txt'%('encode',sub,ses,run,hem))\n",
    "        if ses != 'ses-03':\n",
    "            encode_pairs = [recall_sheet['loci'][r]+'-'+recall_sheet['encode'][r] for r in range(len(recall_sheet))][:40]\n",
    "        else:\n",
    "            encode_pairs = [recall_sheet['loci'][r]+'-'+recall_sheet['encode'][r] for r in range(len(recall_sheet))][:20]\n",
    "        valid_encode_pairs = [p for p in encode_pairs if p[0]!='x']\n",
    "        for r, pair in enumerate(valid_encode_pairs):\n",
    "            encode_beta_dict[hem][pair.lower()] = encode_fmri[:,r]\n",
    "        ret_idx = 0\n",
    "        for r in range(len(recall_sheet)):\n",
    "            if recall_sheet['spoken_loci'][r]!='x':\n",
    "                retrieve_beta_dict[hem][(recall_sheet['spoken_loci'][r]+'-'+recall_sheet['retrieval'][r]).lower()] = recall_fmri[:,ret_idx]\n",
    "                ret_idx+=1\n",
    "    return loci_beta_dict, item_beta_dict, encode_beta_dict, retrieve_beta_dict\n",
    "\n",
    "\n",
    "def get_residuals(C, dv):\n",
    "    \"\"\"\n",
    "    Returns the residuals of the dv(th) column of matrix C, accounting for all variances explained by other columns\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n, p)\n",
    "        Array with the different variables. Each column of C is taken as a variable\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : array-like, shape (n,)\n",
    "        Residual of dv\n",
    "    \"\"\"\n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    idx = np.ones(p, dtype=np.bool)\n",
    "    idx[dv] = False\n",
    "    beta = linalg.lstsq(C[:, idx], C[:, dv])[0]\n",
    "\n",
    "    res = C[:, dv] - C[:, idx].dot(beta)\n",
    "            \n",
    "    return res\n",
    "\n",
    "\n",
    "def get_residual(target, var1, var2):\n",
    "    # Create a 2D array for the independent variables\n",
    "    X = np.column_stack((var1, var2))\n",
    "    \n",
    "    # Fit a linear regression model with the independent variables\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, target)\n",
    "    \n",
    "    # Get the predicted values for the target variable based on the model\n",
    "    predicted = model.predict(X)\n",
    "    \n",
    "    # Calculate the residuals by subtracting the predicted values from the original target values\n",
    "    residuals = target - predicted\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "\n",
    "def calculate_weights(x, y, z, A):\n",
    "    \"\"\"\n",
    "    Calculate the weights of variables x, y, z in predicting each timepoint of matrix A.\n",
    "\n",
    "    Parameters:\n",
    "    x, y, z: Arrays of shape (nv,) representing the variables in each vertices in the ROI.\n",
    "    A: Matrix of shape (nv, nTR) where each column is a timepoint.\n",
    "\n",
    "    Returns:\n",
    "    weights: A matrix of shape (nTR, 3) containing the weights for each variable for each timepoint.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine x, y, z into a single matrix\n",
    "    predictors = np.column_stack((x, y, z))\n",
    "    try:\n",
    "        num_TRs = A.shape[1]\n",
    "            # Initialize an array to store weights\n",
    "        weights = np.zeros((num_TRs, 3))\n",
    "\n",
    "        # Loop over each timepoint\n",
    "        for i in range(num_TRs):\n",
    "            # Create a linear regression model\n",
    "            model = LinearRegression()\n",
    "\n",
    "            # Fit the model\n",
    "            model.fit(predictors, A[:, i])\n",
    "\n",
    "            # Store the weights\n",
    "            weights[i, :] = model.coef_\n",
    "    except:\n",
    "        num_TRs = 1\n",
    "        model = LinearRegression()\n",
    "        model.fit(predictors, A)\n",
    "        weights = model.coef_\n",
    "    \n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shuffled_indexes(indexes_to_shuffle, num_permutation, seed=None):\n",
    "    random_index_container = []\n",
    "    for p in range(num_permutation):\n",
    "        random.seed(seed+str(p))\n",
    "        shuffled_list = indexes_to_shuffle[:]\n",
    "        random.shuffle(shuffled_list)\n",
    "        random_index_container.append(shuffled_list)\n",
    "    return random_index_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_matrix(sub, seses = ['ses-01','ses-02','ses-03'],expert = False):\n",
    "    # generate a matrix with each locus-item pair as a row, with a bunch of measures on similarities\n",
    "    # in neural activity, univariate activity, and story deviation, for different ROIs,\n",
    "    # the sheet is used for subsequent analyses in R.\n",
    "    for ses in seses:\n",
    "        recall_sheet = pd.read_excel('../updated_sheets/%s_recallperformance.xlsx'%sub2subj[sub], sheet_name=ses2w[ses].lower())\n",
    "\n",
    "        all_dist = {\"encode_pair\":[],\"retrieve_pair\":[], \"story\":[], \"story_deviation\":[],\"story_deviation_locus\":[],\"story_deviation_item\":[],'speak_duration':[],'sub':[],'ses':[],\n",
    "                    'e_i_sim_ag':[],'e_i_sim_pmc':[],'e_i_sim_mpfc':[],\n",
    "                    'e_l_sim_ag':[],'e_l_sim_pmc':[],'e_l_sim_mpfc':[],\n",
    "                    'e_r_sim_ag':[],'e_r_sim_pmc':[],'e_r_sim_mpfc':[],\n",
    "                    'e_li_sim_ag':[],'e_li_sim_pmc':[],'e_li_sim_mpfc':[],\n",
    "                    'encode_ag_univariate':[],'encode_pmc_univariate':[],'encode_mpfc_univariate':[],'retrieve_ag_univariate':[],'retrieve_pmc_univariate':[],'retrieve_mpfc_univariate':[],\n",
    "                    'locus_ag_univariate':[],'locus_pmc_univariate':[],'locus_mpfc_univariate':[],'item_ag_univariate':[],'item_pmc_univariate':[],'item_mpfc_univariate':[],\n",
    "                    \"encode_anterior_hipp_univariate\":[], \"encode_posterior_hipp_univariate\":[], \"encode_hippo_univariate\":[],\n",
    "                    'locus_anterior_hipp_univariate':[], 'locus_posterior_hipp_univariate':[], 'locus_hippo_univariate':[],\n",
    "                    'item_anterior_hipp_univariate':[], '   item_posterior_hipp_univariate':[], 'item_hippo_univariate':[],\n",
    "                    'retrieve_anterior_hipp_univariate':[], 'retrieve_posterior_hipp_univariate':[], 'retrieve_hippo_univariate':[],\n",
    "                    'e_r_sim_residual_ag':[],'e_r_sim_residual_pmc':[],'e_r_sim_residual_mpfc':[],\n",
    "                    'beta_e_residual_ag':[],'beta_e_residual_pmc':[],'beta_e_residual_mpfc':[],\n",
    "                    'beta_e_residual_z_ag':[], 'beta_e_residual_z_pmc':[], 'beta_e_residual_z_mpfc':[],\n",
    "                    'beta_i_z_ag':[], 'beta_i_z_pmc':[], 'beta_i_z_mpfc':[],\n",
    "                    'beta_l_z_ag':[], 'beta_l_z_pmc':[], 'beta_l_z_mpfc':[],\n",
    "                    'beta_e_residual_posterior_hipp':[], 'beta_e_residual_anterior_hipp':[], 'beta_e_residual_hippo':[],\n",
    "                    'beta_e_residual_z_posterior_hipp':[], 'beta_e_residual_z_anterior_hipp':[], 'beta_e_residual_z_hippo':[],\n",
    "                    'beta_i_z_posterior_hipp':[], 'beta_i_z_anterior_hipp':[], 'beta_i_z_hippo':[],\n",
    "                    'beta_l_z_posterior_hipp':[], 'beta_l_z_anterior_hipp':[], 'beta_l_z_hippo':[],\n",
    "                    'r_i_sim_ag':[],'r_i_sim_pmc':[],'r_i_sim_mpfc':[],\n",
    "                    'r_l_sim_ag':[],'r_l_sim_pmc':[],'r_l_sim_mpfc':[],\n",
    "                    'correct':[]}\n",
    "\n",
    "        loci_beta_dict, item_beta_dict, encode_beta_dict, retrieve_beta_dict = get_beta_dicts(sub,ses, expert = expert)\n",
    "        encode_loci_list = [l.lower() for l in list(recall_sheet['loci'])]\n",
    "        valid_pairs = []\n",
    "        \n",
    "        idxes_in_valid = []\n",
    "        for i in range(len(recall_sheet)):\n",
    "            retrieve_pair = recall_sheet['spoken_loci'][i].lower()+'-'+recall_sheet['retrieval'][i].lower()\n",
    "            locus, item = retrieve_pair.split('-')\n",
    "            if retrieve_pair in encode_beta_dict['anterior_hipp'].keys() and locus in loci_beta_dict['anterior_hipp'].keys():\n",
    "                valid_pairs.append(retrieve_pair)\n",
    "                idxes_in_valid.append(i)\n",
    "\n",
    "                \n",
    "        shuffled_keys = generate_shuffled_indexes(valid_pairs, 1000, sub+ses)\n",
    "        \n",
    "        for i in range(len(recall_sheet)):\n",
    "            \n",
    "            retrieve_pair = recall_sheet['spoken_loci'][i].lower()+'-'+recall_sheet['retrieval'][i].lower()\n",
    "            locus, item = retrieve_pair.split('-')\n",
    "            valid_pair = False\n",
    "            if retrieve_pair in encode_beta_dict['anterior_hipp'].keys() and locus in loci_beta_dict['anterior_hipp'].keys():\n",
    "                valid_pair = True\n",
    "                encode_pair = retrieve_pair\n",
    "                all_dist['retrieve_pair'].append(retrieve_pair.lower())\n",
    "                all_dist['encode_pair'].append(retrieve_pair.lower())\n",
    "                all_dist['correct'].append(True)\n",
    "                \n",
    "\n",
    "            elif item == 'x' and locus in encode_loci_list and locus in loci_beta_dict['anterior_hipp'].keys():\n",
    "                valid_pair = True\n",
    "                locus_index_encoding = encode_loci_list.index(locus)\n",
    "                item = recall_sheet['encode'][locus_index_encoding]\n",
    "                encode_pair = locus.lower()+'-'+item.lower()\n",
    "                all_dist['retrieve_pair'].append(retrieve_pair.lower())\n",
    "                all_dist['encode_pair'].append(encode_pair.lower())\n",
    "                all_dist['correct'].append(False)\n",
    "                \n",
    "                \n",
    "            if valid_pair:\n",
    "                try:\n",
    "                    story = recall_sheet.iloc[i,7]\n",
    "                    all_dist['story'].append(story)\n",
    "                    all_dist['speak_duration'].append(recall_sheet['elapsed'][i])\n",
    "                    all_dist['story_deviation'].append(getSentenceSimilarity(encode_pair,story))\n",
    "                    all_dist['story_deviation_locus'].append(getSentenceSimilarity(locus,story))\n",
    "                    all_dist['story_deviation_item'].append(getSentenceSimilarity(item,story))\n",
    "                except Exception as error:\n",
    "                    all_dist['story'].append(np.nan)\n",
    "                    all_dist['speak_duration'].append(np.nan)\n",
    "                    all_dist['story_deviation'].append(np.nan)\n",
    "                    all_dist['story_deviation_locus'].append(np.nan)\n",
    "                    all_dist['story_deviation_item'].append(np.nan)\n",
    "                \n",
    "                for roi in ROIs:\n",
    "                    roi_l,roi_r = ROIs[roi][0], ROIs[roi][1]\n",
    "                    encode_rep = np.concatenate((encode_beta_dict['lh'][encode_pair][roi_l], encode_beta_dict['rh'][encode_pair][roi_r]))\n",
    "                    locus_rep = np.concatenate((loci_beta_dict['lh'][locus][roi_l],loci_beta_dict['rh'][locus][roi_r]))\n",
    "                    item_rep = np.concatenate((item_beta_dict['lh'][item][roi_l],item_beta_dict['rh'][item][roi_r]))\n",
    "                    retrieve_rep = np.concatenate((retrieve_beta_dict['lh'][retrieve_pair][roi_l],retrieve_beta_dict['rh'][retrieve_pair][roi_r]))\n",
    "                    \n",
    "                    locus_item_average = (locus_rep+item_rep)/2\n",
    "                    \n",
    "                    e_residual = get_residual(encode_rep,locus_rep,item_rep)\n",
    "                    r_residual = get_residual(retrieve_rep,locus_rep,item_rep)\n",
    "\n",
    "                    l_weight,i_weight,e_weight= calculate_weights(locus_rep, item_rep, e_residual, retrieve_rep)\n",
    "\n",
    "                    loci_weights_shuffled_item, item_weights_shuffled_item, encode_weights_shuffled_item = [],[],[]\n",
    "                    try:\n",
    "                        for p in range(1000):\n",
    "                            shuffled_key = shuffled_keys[p][idxes_in_valid.index(i)]\n",
    "                            retrieve_rep_shuffled = np.concatenate((retrieve_beta_dict['lh'][shuffled_key][roi_l],retrieve_beta_dict['rh'][shuffled_key][roi_r]))\n",
    "                            l_weights_shuffled,i_weights_shuffled,e_weights_shuffled = calculate_weights(locus_rep, item_rep, e_residual, retrieve_rep_shuffled)\n",
    "                            loci_weights_shuffled_item.append(l_weights_shuffled)\n",
    "                            item_weights_shuffled_item.append(i_weights_shuffled)\n",
    "                            encode_weights_shuffled_item.append(e_weights_shuffled)\n",
    "                        \n",
    "                        all_dist[f'beta_e_residual_z_{roi}'].append((e_weight-np.mean(encode_weights_shuffled_item))/np.std(encode_weights_shuffled_item))\n",
    "                        all_dist[f'beta_l_z_{roi}'].append((l_weight-np.mean(loci_weights_shuffled_item))/np.std(loci_weights_shuffled_item))\n",
    "                        all_dist[f'beta_i_z_{roi}'].append((i_weight-np.mean(item_weights_shuffled_item))/np.std(item_weights_shuffled_item))\n",
    "                    \n",
    "                    except: \n",
    "                        all_dist[f'beta_e_residual_z_{roi}'].append(np.nan)\n",
    "                        all_dist[f'beta_l_z_{roi}'].append(np.nan)\n",
    "                        all_dist[f'beta_i_z_{roi}'].append(np.nan)\n",
    "\n",
    "                    e_i_similarity = stats.pearsonr(encode_rep,item_rep)[0]\n",
    "                    e_l_similarity = stats.pearsonr(encode_rep,locus_rep)[0]\n",
    "                    e_r_similarity = stats.pearsonr(encode_rep,retrieve_rep)[0]\n",
    "                    e_li_similarity = stats.pearsonr(encode_rep,locus_item_average)[0]\n",
    "                    e_r_similarity_residual = stats.pearsonr(e_residual,r_residual)[0]\n",
    "                    r_i_similarity = stats.pearsonr(retrieve_rep,item_rep)[0]\n",
    "                    r_l_similarity = stats.pearsonr(retrieve_rep,locus_rep)[0]\n",
    "                    all_dist[f'item_{roi}_univariate'].append(np.mean(item_rep))\n",
    "                    all_dist[f'locus_{roi}_univariate'].append(np.mean(locus_rep))\n",
    "                    all_dist[f'encode_{roi}_univariate'].append(np.mean(encode_rep))\n",
    "                    all_dist[f'retrieve_{roi}_univariate'].append(np.mean(retrieve_rep))\n",
    "\n",
    "                    all_dist[f'e_i_sim_{roi}'].append(e_i_similarity)\n",
    "                    all_dist[f'e_l_sim_{roi}'].append(e_l_similarity)\n",
    "                    all_dist[f'e_r_sim_{roi}'].append(e_r_similarity)\n",
    "                    all_dist[f'e_r_sim_residual_{roi}'].append(e_r_similarity_residual)\n",
    "                    all_dist[f'r_l_sim_{roi}'].append(r_l_similarity)\n",
    "                    all_dist[f'r_i_sim_{roi}'].append(r_i_similarity)\n",
    "                    all_dist[f'e_li_sim_{roi}'].append(e_li_similarity)\n",
    "                    all_dist[f'beta_e_residual_{roi}'].append(e_weight)\n",
    "        \n",
    "\n",
    "                for roi in hippo_ROIs:\n",
    "                    # retrieve_pair = all_dist['retrieve_pair'][pair_idx]\n",
    "                    # encode_pair = all_dist['encode_pair'][pair_idx]\n",
    "                    # locus, _ = retrieve_pair.split('-')\n",
    "                    # _, item = encode_pair.split('-')\n",
    "                    encode_rep = encode_beta_dict[roi][encode_pair]\n",
    "                    locus_rep = loci_beta_dict[roi][locus]\n",
    "                    item_rep = item_beta_dict[roi][item]\n",
    "                    retrieve_rep = retrieve_beta_dict[roi][retrieve_pair]                \n",
    "                    \n",
    "                    e_residual = get_residual(encode_rep,locus_rep,item_rep)\n",
    "                    r_residual = get_residual(retrieve_rep,locus_rep,item_rep)\n",
    "                    l_weight,i_weight,e_weight= calculate_weights(locus_rep, item_rep, e_residual, retrieve_rep)\n",
    "\n",
    "                    loci_weights_shuffled_item, item_weights_shuffled_item, encode_weights_shuffled_item = [],[],[]\n",
    "                    try:\n",
    "                        for p in range(1000):\n",
    "                            shuffled_key = shuffled_keys[p][idxes_in_valid.index(i)]\n",
    "                            retrieve_rep_shuffled = retrieve_beta_dict[roi][shuffled_key]\n",
    "                            l_weights_shuffled,i_weights_shuffled,e_weights_shuffled = calculate_weights(locus_rep, item_rep, e_residual, retrieve_rep_shuffled)\n",
    "                            loci_weights_shuffled_item.append(l_weights_shuffled)\n",
    "                            item_weights_shuffled_item.append(i_weights_shuffled)\n",
    "                            encode_weights_shuffled_item.append(e_weights_shuffled)\n",
    "                        \n",
    "                        all_dist[f'beta_e_residual_z_{roi}'].append((e_weight-np.mean(encode_weights_shuffled_item))/np.std(encode_weights_shuffled_item))\n",
    "                        all_dist[f'beta_l_z_{roi}'].append((l_weight-np.mean(loci_weights_shuffled_item))/np.std(loci_weights_shuffled_item))\n",
    "                        all_dist[f'beta_i_z_{roi}'].append((i_weight-np.mean(item_weights_shuffled_item))/np.std(item_weights_shuffled_item))\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        all_dist[f'beta_e_residual_z_{roi}'].append(np.nan)\n",
    "                        all_dist[f'beta_l_z_{roi}'].append(np.nan)\n",
    "                        all_dist[f'beta_i_z_{roi}'].append(np.nan)\n",
    "\n",
    "                    all_dist[f'beta_e_residual_{roi}'].append(e_weight)\n",
    "\n",
    "                    all_dist[f'encode_{roi}_univariate'].append(np.mean(encode_rep))\n",
    "                    all_dist[f'retrieve_{roi}_univariate'].append(np.mean(retrieve_rep))\n",
    "                    all_dist[f'item_{roi}_univariate'].append(np.mean(item_rep))\n",
    "                    all_dist[f'locus_{roi}_univariate'].append(np.mean(locus_rep))\n",
    "\n",
    "        all_dist['sub'] = sub\n",
    "        all_dist['ses'] = ses\n",
    "        all_dist = pd.DataFrame(all_dist)\n",
    "        all_dist.to_csv(f'../outputs/story_matrix/{sub}_{ses}_story_matrix_withHipp.csv',index=False)\n",
    "    # except Exception as error:\n",
    "    #     print(error)\n",
    "    #     print(f'{sub} {ses} not found')\n",
    "    #     continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Parallel(n_jobs=13)(delayed(get_story_matrix)(sub) for sub in subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the matrices\n",
    "story_matrices = []\n",
    "for sub in subjects:\n",
    "    for ses in sessions:\n",
    "        story_matrix = pd.read_csv(f'../outputs/story_matrix/{sub}_{ses}_story_matrix_withHipp.csv')\n",
    "        story_matrices.append(story_matrix)\n",
    "story_matrix = pd.concat(story_matrices)\n",
    "story_matrix.to_csv('../outputs/story_matrix_withHipp.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv Environment",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
