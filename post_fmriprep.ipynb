{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import os\n",
    "import h5py\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "#path to data (change to your path)\n",
    "fmripreppath = '/data/MoL_clean/fmriprep/'\n",
    "\n",
    "#path to output (change to your path)\n",
    "prepath = '/data/MoL_clean/preprocessed/'\n",
    "\n",
    "#list of sessions\n",
    "sesslist=['ses-01','ses-02','ses-03']\n",
    "\n",
    "#list of tasks/#runs in each session\n",
    "tasklist = [[('Loci', 2),('Item',2),('Encode',1),('Retrieve',1)],\n",
    "            [('Loci', 2),('Item',2),('Encode',1),('Retrieve',1)],\n",
    "            [('Loci', 2),('Item',2),('Encode',1),('Retrieve',1)]]\n",
    "\n",
    "#list of subjects\n",
    "subs= ['sub-%02d' % i for i in range(1,26)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(os.path.exists(sess_prefix + '_task-' + task[0] + ('_run-%.2d' % offset) + '_desc-confounds_timeseries.tsv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify group masks for hippocampus\n",
    "regional_mask_filenames = sorted([f for f in glob.glob(fmripreppath+'*/ses-01/func/*_ses-01_task-Loci_run-01_space-MNI152NLin2009cAsym_desc-aseg_dseg.nii.gz') ])\n",
    "regional_mask_filenames = regional_mask_filenames[:10]+regional_mask_filenames[16:]\n",
    "regional_masks = []\n",
    "hippo_masks = []\n",
    "for regional_mask_fname in regional_mask_filenames:\n",
    "    regional_mask = nib.load(regional_mask_fname).get_fdata()\n",
    "    regional_masks.append(regional_mask)\n",
    "    hippo_masks.append(np.where(np.logical_or(regional_mask == 17, regional_mask == 53)))\n",
    "\n",
    "hippo_group_mask_template = np.zeros(regional_mask.shape)\n",
    "hippo_group_mask = np.zeros(regional_mask.shape)\n",
    "for hippo_mask in hippo_masks:\n",
    "    hippo_group_mask_template[hippo_mask] += 1  \n",
    "hippo_group_mask[hippo_group_mask_template==25] = 1\n",
    "\n",
    "anterior_group,posterior_group = hippo_group_mask.copy(),hippo_group_mask.copy()\n",
    "anterior_group[:,55:,:] = False\n",
    "posterior_group[:,:55,:] = False\n",
    "anterior_group = anterior_group.astype(bool)\n",
    "posterior_group = posterior_group.astype(bool)\n",
    "hippo_group_mask = hippo_group_mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for sub in subs:\n",
    "    print('Processing subject:', sub)\n",
    "\n",
    "    for sess_ind, sess in enumerate(sesslist):\n",
    "        print('session:', sess)\n",
    "\n",
    "        if sess == '':\n",
    "            sess_prefix = os.path.join(fmripreppath, sub, sess, 'func', sub)\n",
    "        else:\n",
    "            sess_prefix = os.path.join(fmripreppath, sub, sess, 'func', sub + '_' + sess)\n",
    "\n",
    "        sess_tasknames = []\n",
    "        for task in tasklist[sess_ind]:\n",
    "            if task[1] == 1:\n",
    "                sess_tasknames.append(task[0]+'_run-01')\n",
    "            else:\n",
    "                # For run numbers that don't start at zero\n",
    "                offset = 0\n",
    "                while not os.path.exists(sess_prefix + '_task-' + task[0] + ('_run-%.2d' % offset) + '_desc-confounds_timeseries.tsv'):\n",
    "                    print('looping')\n",
    "                    offset += 1\n",
    "                    if offset>10:\n",
    "                        print(\"something is wrong\")\n",
    "                        break\n",
    "                for r in range(task[1]):\n",
    "                    sess_tasknames.append('%s_run-%.2d' % (task[0], r+offset))\n",
    "        for task_name in sess_tasknames:\n",
    "            print('task:', task_name)\n",
    "\n",
    "            task_prefix = sess_prefix + '_task-' + task_name\n",
    "\n",
    "            D = dict()\n",
    "            for hem in ['L', 'R', 'Vol']:\n",
    "                if hem == 'Vol':\n",
    "                    # Load all timecourses in the 3d brain mask\n",
    "\n",
    "                    fname =  task_prefix + '_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "                    print('      Loading ', fname)\n",
    "                    \n",
    "                    nii_4d = nib.load(fname).get_fdata()\n",
    "\n",
    "                    mask_fname = task_prefix + '_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz'\n",
    "                    mask_3d = nib.load(mask_fname).get_fdata().astype(bool)\n",
    "                    \n",
    "                    regional_mask_fname = task_prefix + '_space-MNI152NLin2009cAsym_desc-aseg_dseg.nii.gz'\n",
    "                    regional_mask = nib.load(regional_mask_fname).get_fdata()\n",
    "                    hippo_mask = np.logical_or(regional_mask == 17, regional_mask == 53)\n",
    "                    anterior,posterior = hippo_mask.copy(),hippo_mask.copy()\n",
    "                    anterior[:,55:,:] = False\n",
    "                    posterior[:,:55,:] = False\n",
    "                    D['anterior_hipp'] = nii_4d[anterior]\n",
    "                    D['posterior_hipp'] = nii_4d[posterior]\n",
    "                    D['hippo'] = nii_4d[hippo_mask]\n",
    "                    D['anterior_hipp_group'] = nii_4d[anterior_group]\n",
    "                    D['posterior_hipp_group'] = nii_4d[posterior_group]\n",
    "                    D['hippo_group'] = nii_4d[hippo_group_mask]\n",
    "                    D[hem] = nii_4d[mask_3d]\n",
    "                    \n",
    "                else:\n",
    "                    # Load all timecourses for one cortical hemisphere\n",
    "\n",
    "                    fname = task_prefix +'_hemi-'+hem+'_space-fsaverage6_bold.func.gii'\n",
    "                    print('      Loading ', fname)\n",
    "\n",
    "                    gi = nib.load(fname)\n",
    "                    D[hem] = np.column_stack([gi.darrays[t].data for t in range(len(gi.darrays))])\n",
    "\n",
    "\n",
    "            # Load confound regressors\n",
    "            conf = np.genfromtxt(task_prefix + '_desc-confounds_timeseries.tsv', names=True)\n",
    "\n",
    "            conf_json = json.load(open(task_prefix + '_desc-confounds_timeseries.json'))\n",
    "\n",
    "            # Find first combined compcor regressor\n",
    "            first_cc = 0\n",
    "            while True:\n",
    "                if conf_json['a_comp_cor_%02d' % first_cc]['Mask'] == 'combined':\n",
    "                    break\n",
    "                first_cc += 1\n",
    "\n",
    "            reg = np.column_stack((\n",
    "\n",
    "                # Motion and motion derivatives\n",
    "                conf['trans_x'],\n",
    "                conf['trans_x_derivative1'],\n",
    "                conf['trans_y'],\n",
    "                conf['trans_y_derivative1'],\n",
    "                conf['trans_z'],\n",
    "                conf['trans_z_derivative1'],\n",
    "                conf['rot_x'],\n",
    "                conf['rot_x_derivative1'],\n",
    "                conf['rot_y'],\n",
    "                conf['rot_y_derivative1'],\n",
    "                conf['rot_z'],\n",
    "                conf['rot_z_derivative1'],\n",
    "                conf['framewise_displacement'],\n",
    "\n",
    "                # First six compcor components (white matter + CSF signals)\n",
    "                conf['a_comp_cor_%02d' % first_cc],\n",
    "                conf['a_comp_cor_%02d' % (first_cc+1)],\n",
    "                conf['a_comp_cor_%02d' % (first_cc+2)],\n",
    "                conf['a_comp_cor_%02d' % (first_cc+3)],\n",
    "                conf['a_comp_cor_%02d' % (first_cc+4)],\n",
    "                conf['a_comp_cor_%02d' % (first_cc+5)],\n",
    "\n",
    "                # Cosine (drift) and motion spikes\n",
    "                np.column_stack([conf[k] for k in conf.dtype.names if ('cosine' in k) or ('motion_outlier' in k)])))\n",
    "\n",
    "            # Remove nans, e.g. from framewise_displacement\n",
    "            reg = np.nan_to_num(reg)\n",
    "\n",
    "            print('      Cleaning and zscoring')\n",
    "            for hem in ['L', 'R', 'Vol', 'anterior_hipp','posterior_hipp','hippo','anterior_hipp_group','posterior_hipp_group','hippo_group']:\n",
    "                # Regress out confounds from data\n",
    "                regr = linear_model.LinearRegression()\n",
    "                regr.fit(reg, D[hem].T)\n",
    "                D[hem] = D[hem] - np.dot(regr.coef_, reg.T) - regr.intercept_[:, np.newaxis]\n",
    "                # Note 8% of values on cortical surface are NaNs, and the following will therefore throw an error\n",
    "                D[hem] = stats.zscore(D[hem], axis=1)\n",
    "\n",
    "            # Save hdf5 file\n",
    "            if sess == '':\n",
    "                savepath = os.path.join(prepath, sub + '_' + task_name +  '.h5')\n",
    "            else:\n",
    "                savepath = os.path.join(prepath, sub + '_' + sess + '_' + task_name +  '.h5')\n",
    "            with h5py.File(savepath,'w') as hf:\n",
    "                grp = hf.create_group(task_name)\n",
    "                grp.create_dataset('L', data=D['L'])\n",
    "                grp.create_dataset('R', data=D['R'])\n",
    "                grp.create_dataset('Vol', data=D['Vol'])\n",
    "                grp.create_dataset('anterior_hipp', data=D['anterior_hipp'])\n",
    "                grp.create_dataset('posterior_hipp', data=D['posterior_hipp'])\n",
    "                grp.create_dataset('hippo', data=D['hippo'])\n",
    "                grp.create_dataset('anterior_hipp_group', data=D['anterior_hipp_group'])\n",
    "                grp.create_dataset('posterior_hipp_group', data=D['posterior_hipp_group'])\n",
    "                grp.create_dataset('hippo_group', data=D['hippo_group'])\n",
    "\n",
    "                grp.create_dataset('reg',data=reg)\n",
    "                grp.create_dataset('mask', data=mask_3d)\n",
    "                grp.create_dataset('hippo_mask', data=hippo_mask)\n",
    "\n",
    "            print('      saved hdf5 file')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Venv Environment",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
